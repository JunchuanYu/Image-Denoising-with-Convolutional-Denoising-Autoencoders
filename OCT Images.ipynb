{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import skimage\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from skimage import data, img_as_float\n",
    "from skimage.restoration import denoise_nl_means\n",
    "from skimage.measure import compare_ssim\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from skimage.measure import compare_psnr,compare_mse\n",
    "from scipy import ndimage, misc\n",
    "from random import shuffle\n",
    "import math\n",
    "from keras.models import model_from_json, load_model\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "import re\n",
    "from keras import regularizers\n",
    "from keras.utils import plot_model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras import backend as K\n",
    "from keras.callbacks import TensorBoard, ReduceLROnPlateau, CSVLogger, EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.core import Lambda\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.losses import mean_squared_error\n",
    "\n",
    "\n",
    "import pickle\n",
    "from random import randint\n",
    "from utilities import *\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "\n",
    "    r, g, b = rgb[:,:,:,0], rgb[:,:,:,1], rgb[:,:,:,2]\n",
    "    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "\n",
    "    return gray\n",
    "\n",
    "def read_images_from_folder(folder_path):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        img = cv2.imread(os.path.join(folder_path,filename),0)\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    return np.array(images)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.tif\n",
      "1.tif\n",
      "2.tif\n",
      "3.tif\n",
      "4.tif\n",
      "5.tif\n",
      "6.tif\n",
      "7.tif\n",
      "8.tif\n",
      "9.tif\n",
      "10.tif\n",
      "11.tif\n",
      "12.tif\n",
      "13.tif\n",
      "14.tif\n",
      "15.tif\n",
      "16.tif\n",
      "17.tif\n",
      "18.tif\n",
      "19.tif\n",
      "20.tif\n",
      "21.tif\n",
      "22.tif\n",
      "23.tif\n",
      "24.tif\n",
      "25.tif\n",
      "26.tif\n",
      "27.tif\n",
      "28.tif\n",
      "29.tif\n",
      "30.tif\n",
      "31.tif\n",
      "32.tif\n",
      "33.tif\n",
      "34.tif\n",
      "35.tif\n",
      "36.tif\n",
      "37.tif\n",
      "38.tif\n",
      "39.tif\n",
      "40.tif\n",
      "41.tif\n",
      "42.tif\n",
      "43.tif\n",
      "44.tif\n",
      "45.tif\n",
      "46.tif\n",
      "47.tif\n",
      "48.tif\n",
      "49.tif\n",
      "50.tif\n",
      "51.tif\n",
      "52.tif\n",
      "53.tif\n",
      "54.tif\n",
      "55.tif\n",
      "56.tif\n",
      "57.tif\n",
      "58.tif\n",
      "59.tif\n",
      "60.tif\n",
      "61.tif\n",
      "62.tif\n",
      "63.tif\n",
      "64.tif\n",
      "65.tif\n",
      "66.tif\n",
      "67.tif\n",
      "68.tif\n",
      "69.tif\n",
      "70.tif\n",
      "71.tif\n",
      "72.tif\n",
      "73.tif\n",
      "74.tif\n",
      "75.tif\n",
      "76.tif\n",
      "77.tif\n",
      "78.tif\n",
      "79.tif\n",
      "80.tif\n",
      "81.tif\n",
      "82.tif\n",
      "83.tif\n",
      "84.tif\n",
      "85.tif\n",
      "86.tif\n",
      "87.tif\n",
      "88.tif\n",
      "89.tif\n",
      "90.tif\n",
      "91.tif\n",
      "92.tif\n",
      "93.tif\n",
      "94.tif\n",
      "95.tif\n",
      "96.tif\n",
      "97.tif\n",
      "98.tif\n",
      "99.tif\n",
      "100.tif\n",
      "101.tif\n",
      "102.tif\n",
      "103.tif\n",
      "104.tif\n",
      "105.tif\n",
      "106.tif\n",
      "107.tif\n",
      "108.tif\n",
      "109.tif\n",
      "110.tif\n",
      "111.tif\n",
      "112.tif\n",
      "113.tif\n",
      "114.tif\n",
      "115.tif\n",
      "116.tif\n",
      "117.tif\n",
      "118.tif\n",
      "119.tif\n",
      "120.tif\n",
      "121.tif\n",
      "122.tif\n",
      "123.tif\n",
      "124.tif\n",
      "125.tif\n",
      "126.tif\n",
      "127.tif\n",
      "128.tif\n",
      "129.tif\n",
      "130.tif\n",
      "131.tif\n",
      "132.tif\n",
      "133.tif\n",
      "134.tif\n",
      "135.tif\n",
      "136.tif\n",
      "137.tif\n",
      "138.tif\n",
      "139.tif\n",
      "140.tif\n",
      "141.tif\n",
      "142.tif\n",
      "143.tif\n",
      "144.tif\n",
      "145.tif\n",
      "146.tif\n",
      "147.tif\n",
      "148.tif\n",
      "149.tif\n",
      "150.tif\n",
      "151.tif\n",
      "152.tif\n",
      "153.tif\n",
      "154.tif\n",
      "155.tif\n",
      "156.tif\n",
      "157.tif\n",
      "158.tif\n",
      "159.tif\n",
      "160.tif\n",
      "161.tif\n",
      "162.tif\n",
      "163.tif\n",
      "164.tif\n",
      "165.tif\n",
      "166.tif\n",
      "167.tif\n",
      "168.tif\n",
      "169.tif\n",
      "170.tif\n",
      "171.tif\n",
      "172.tif\n",
      "173.tif\n",
      "174.tif\n",
      "175.tif\n",
      "176.tif\n",
      "177.tif\n",
      "178.tif\n",
      "179.tif\n",
      "180.tif\n",
      "181.tif\n",
      "182.tif\n",
      "183.tif\n",
      "184.tif\n",
      "185.tif\n",
      "186.tif\n",
      "187.tif\n",
      "188.tif\n",
      "189.tif\n",
      "190.tif\n",
      "191.tif\n",
      "192.tif\n",
      "193.tif\n",
      "194.tif\n",
      "195.tif\n",
      "196.tif\n",
      "197.tif\n",
      "198.tif\n",
      "199.tif\n",
      "200.tif\n",
      "201.tif\n",
      "202.tif\n",
      "203.tif\n",
      "204.tif\n",
      "205.tif\n",
      "206.tif\n",
      "207.tif\n",
      "208.tif\n",
      "209.tif\n",
      "210.tif\n",
      "211.tif\n",
      "212.tif\n",
      "213.tif\n",
      "214.tif\n",
      "215.tif\n",
      "216.tif\n",
      "217.tif\n",
      "218.tif\n",
      "219.tif\n",
      "220.tif\n",
      "221.tif\n",
      "222.tif\n",
      "223.tif\n",
      "224.tif\n",
      "225.tif\n",
      "226.tif\n",
      "227.tif\n",
      "228.tif\n",
      "229.tif\n",
      "230.tif\n",
      "231.tif\n",
      "232.tif\n",
      "233.tif\n",
      "234.tif\n",
      "235.tif\n",
      "236.tif\n",
      "237.tif\n",
      "238.tif\n",
      "239.tif\n",
      "240.tif\n",
      "241.tif\n",
      "242.tif\n",
      "243.tif\n",
      "244.tif\n",
      "245.tif\n",
      "246.tif\n",
      "247.tif\n",
      "248.tif\n",
      "249.tif\n",
      "250.tif\n",
      "251.tif\n",
      "252.tif\n",
      "253.tif\n",
      "254.tif\n",
      "255.tif\n",
      "256.tif\n",
      "257.tif\n",
      "258.tif\n",
      "259.tif\n",
      "260.tif\n",
      "261.tif\n",
      "262.tif\n",
      "263.tif\n",
      "264.tif\n",
      "265.tif\n",
      "266.tif\n",
      "267.tif\n",
      "268.tif\n",
      "269.tif\n",
      "270.tif\n",
      "271.tif\n",
      "272.tif\n",
      "273.tif\n",
      "274.tif\n",
      "275.tif\n",
      "276.tif\n",
      "277.tif\n",
      "278.tif\n",
      "279.tif\n",
      "280.tif\n",
      "281.tif\n",
      "282.tif\n",
      "283.tif\n",
      "284.tif\n",
      "285.tif\n",
      "286.tif\n",
      "287.tif\n",
      "288.tif\n",
      "289.tif\n",
      "290.tif\n",
      "291.tif\n",
      "292.tif\n",
      "293.tif\n",
      "294.tif\n",
      "295.tif\n",
      "296.tif\n",
      "297.tif\n",
      "298.tif\n",
      "299.tif\n",
      "300.tif\n",
      "301.tif\n",
      "302.tif\n",
      "303.tif\n",
      "304.tif\n",
      "305.tif\n",
      "306.tif\n",
      "307.tif\n",
      "308.tif\n",
      "309.tif\n",
      "310.tif\n",
      "311.tif\n",
      "312.tif\n",
      "313.tif\n",
      "314.tif\n",
      "315.tif\n",
      "316.tif\n",
      "317.tif\n",
      "318.tif\n",
      "319.tif\n",
      "320.tif\n",
      "321.tif\n",
      "322.tif\n",
      "323.tif\n",
      "324.tif\n",
      "325.tif\n",
      "326.tif\n",
      "327.tif\n",
      "328.tif\n",
      "329.tif\n",
      "330.tif\n",
      "331.tif\n",
      "332.tif\n",
      "333.tif\n",
      "334.tif\n",
      "335.tif\n",
      "336.tif\n",
      "337.tif\n",
      "338.tif\n",
      "339.tif\n",
      "340.tif\n",
      "341.tif\n",
      "342.tif\n",
      "343.tif\n",
      "344.tif\n",
      "345.tif\n",
      "346.tif\n",
      "347.tif\n",
      "348.tif\n",
      "349.tif\n",
      "350.tif\n",
      "351.tif\n",
      "352.tif\n",
      "353.tif\n",
      "354.tif\n",
      "355.tif\n",
      "356.tif\n",
      "357.tif\n",
      "358.tif\n",
      "359.tif\n",
      "360.tif\n",
      "361.tif\n",
      "362.tif\n",
      "363.tif\n",
      "364.tif\n",
      "365.tif\n",
      "366.tif\n",
      "367.tif\n",
      "368.tif\n",
      "369.tif\n",
      "370.tif\n",
      "371.tif\n",
      "372.tif\n",
      "373.tif\n",
      "374.tif\n",
      "375.tif\n",
      "376.tif\n",
      "377.tif\n",
      "378.tif\n",
      "379.tif\n",
      "380.tif\n",
      "381.tif\n",
      "382.tif\n",
      "383.tif\n",
      "384.tif\n",
      "385.tif\n",
      "386.tif\n",
      "387.tif\n",
      "388.tif\n",
      "389.tif\n",
      "390.tif\n",
      "391.tif\n",
      "392.tif\n",
      "393.tif\n",
      "394.tif\n",
      "395.tif\n",
      "396.tif\n",
      "397.tif\n",
      "398.tif\n",
      "399.tif\n",
      "400.tif\n",
      "401.tif\n",
      "402.tif\n",
      "403.tif\n",
      "404.tif\n",
      "405.tif\n",
      "406.tif\n",
      "407.tif\n",
      "408.tif\n",
      "409.tif\n",
      "410.tif\n",
      "411.tif\n",
      "412.tif\n",
      "413.tif\n",
      "414.tif\n",
      "415.tif\n",
      "416.tif\n",
      "417.tif\n",
      "418.tif\n",
      "419.tif\n",
      "420.tif\n",
      "421.tif\n",
      "422.tif\n",
      "423.tif\n",
      "424.tif\n",
      "425.tif\n",
      "426.tif\n",
      "427.tif\n",
      "428.tif\n",
      "429.tif\n",
      "430.tif\n",
      "431.tif\n",
      "432.tif\n",
      "433.tif\n",
      "434.tif\n",
      "435.tif\n",
      "436.tif\n",
      "437.tif\n",
      "438.tif\n",
      "439.tif\n",
      "440.tif\n",
      "441.tif\n",
      "442.tif\n",
      "443.tif\n",
      "444.tif\n",
      "445.tif\n",
      "446.tif\n",
      "447.tif\n",
      "448.tif\n",
      "449.tif\n",
      "450.tif\n",
      "451.tif\n",
      "452.tif\n",
      "453.tif\n",
      "454.tif\n",
      "455.tif\n",
      "456.tif\n",
      "457.tif\n",
      "458.tif\n",
      "459.tif\n",
      "460.tif\n",
      "461.tif\n",
      "462.tif\n",
      "463.tif\n",
      "464.tif\n",
      "465.tif\n",
      "466.tif\n",
      "467.tif\n",
      "468.tif\n",
      "469.tif\n",
      "470.tif\n",
      "471.tif\n",
      "472.tif\n",
      "473.tif\n",
      "474.tif\n",
      "475.tif\n",
      "476.tif\n",
      "477.tif\n",
      "478.tif\n",
      "479.tif\n",
      "480.tif\n",
      "481.tif\n",
      "482.tif\n",
      "483.tif\n",
      "484.tif\n",
      "485.tif\n",
      "486.tif\n",
      "487.tif\n",
      "488.tif\n",
      "489.tif\n",
      "490.tif\n",
      "491.tif\n",
      "492.tif\n",
      "493.tif\n",
      "494.tif\n",
      "495.tif\n",
      "496.tif\n",
      "497.tif\n",
      "498.tif\n",
      "499.tif\n",
      "500.tif\n",
      "501.tif\n",
      "502.tif\n",
      "503.tif\n",
      "504.tif\n",
      "505.tif\n",
      "506.tif\n",
      "507.tif\n",
      "508.tif\n",
      "509.tif\n",
      "510.tif\n",
      "511.tif\n",
      "512.tif\n",
      "513.tif\n",
      "514.tif\n",
      "515.tif\n",
      "516.tif\n",
      "517.tif\n",
      "518.tif\n",
      "519.tif\n",
      "520.tif\n",
      "521.tif\n",
      "522.tif\n",
      "523.tif\n",
      "524.tif\n",
      "525.tif\n",
      "526.tif\n",
      "527.tif\n",
      "528.tif\n",
      "529.tif\n",
      "530.tif\n",
      "531.tif\n",
      "532.tif\n",
      "533.tif\n",
      "534.tif\n",
      "535.tif\n",
      "536.tif\n",
      "537.tif\n",
      "538.tif\n",
      "539.tif\n",
      "540.tif\n",
      "541.tif\n",
      "542.tif\n",
      "543.tif\n",
      "544.tif\n",
      "545.tif\n",
      "546.tif\n",
      "547.tif\n",
      "548.tif\n",
      "549.tif\n",
      "550.tif\n",
      "551.tif\n",
      "552.tif\n",
      "553.tif\n",
      "554.tif\n",
      "555.tif\n",
      "556.tif\n",
      "557.tif\n",
      "558.tif\n",
      "559.tif\n",
      "560.tif\n",
      "561.tif\n",
      "562.tif\n",
      "563.tif\n",
      "564.tif\n",
      "565.tif\n",
      "566.tif\n",
      "567.tif\n",
      "568.tif\n",
      "569.tif\n",
      "570.tif\n",
      "571.tif\n",
      "572.tif\n",
      "573.tif\n",
      "574.tif\n",
      "575.tif\n",
      "576.tif\n",
      "577.tif\n",
      "578.tif\n",
      "579.tif\n",
      "580.tif\n",
      "581.tif\n",
      "582.tif\n",
      "583.tif\n",
      "584.tif\n",
      "585.tif\n",
      "586.tif\n",
      "587.tif\n",
      "588.tif\n",
      "589.tif\n",
      "590.tif\n",
      "591.tif\n",
      "592.tif\n",
      "593.tif\n",
      "594.tif\n",
      "595.tif\n",
      "596.tif\n",
      "597.tif\n",
      "598.tif\n",
      "599.tif\n",
      "600.tif\n",
      "601.tif\n",
      "602.tif\n",
      "603.tif\n",
      "604.tif\n",
      "605.tif\n",
      "606.tif\n",
      "607.tif\n",
      "608.tif\n",
      "609.tif\n",
      "610.tif\n",
      "611.tif\n",
      "612.tif\n",
      "613.tif\n",
      "614.tif\n",
      "615.tif\n",
      "616.tif\n",
      "617.tif\n",
      "618.tif\n",
      "619.tif\n",
      "620.tif\n",
      "621.tif\n",
      "622.tif\n",
      "623.tif\n",
      "624.tif\n",
      "625.tif\n",
      "626.tif\n",
      "627.tif\n",
      "628.tif\n",
      "629.tif\n",
      "630.tif\n",
      "631.tif\n",
      "632.tif\n",
      "633.tif\n",
      "634.tif\n",
      "635.tif\n",
      "636.tif\n",
      "637.tif\n",
      "638.tif\n",
      "639.tif\n",
      "640.tif\n",
      "641.tif\n",
      "642.tif\n",
      "643.tif\n",
      "644.tif\n",
      "645.tif\n",
      "646.tif\n",
      "647.tif\n",
      "648.tif\n",
      "649.tif\n",
      "650.tif\n",
      "651.tif\n",
      "652.tif\n",
      "653.tif\n",
      "654.tif\n",
      "655.tif\n",
      "656.tif\n",
      "657.tif\n",
      "658.tif\n",
      "659.tif\n",
      "660.tif\n",
      "661.tif\n",
      "662.tif\n",
      "663.tif\n",
      "664.tif\n",
      "665.tif\n",
      "666.tif\n",
      "667.tif\n",
      "668.tif\n",
      "669.tif\n",
      "670.tif\n",
      "671.tif\n",
      "672.tif\n",
      "673.tif\n",
      "674.tif\n",
      "675.tif\n",
      "676.tif\n",
      "677.tif\n",
      "678.tif\n",
      "679.tif\n",
      "680.tif\n",
      "681.tif\n",
      "682.tif\n",
      "683.tif\n",
      "684.tif\n",
      "685.tif\n",
      "686.tif\n",
      "687.tif\n",
      "688.tif\n",
      "689.tif\n",
      "690.tif\n",
      "691.tif\n",
      "692.tif\n",
      "693.tif\n",
      "694.tif\n",
      "695.tif\n",
      "696.tif\n",
      "697.tif\n",
      "698.tif\n",
      "699.tif\n",
      "700.tif\n",
      "701.tif\n",
      "702.tif\n",
      "703.tif\n",
      "704.tif\n",
      "705.tif\n",
      "706.tif\n",
      "707.tif\n",
      "708.tif\n",
      "709.tif\n",
      "710.tif\n",
      "711.tif\n",
      "712.tif\n",
      "713.tif\n",
      "714.tif\n",
      "715.tif\n",
      "716.tif\n",
      "717.tif\n",
      "718.tif\n",
      "719.tif\n",
      "720.tif\n",
      "721.tif\n",
      "722.tif\n",
      "723.tif\n",
      "724.tif\n",
      "725.tif\n",
      "726.tif\n",
      "727.tif\n",
      "728.tif\n",
      "729.tif\n",
      "730.tif\n",
      "731.tif\n",
      "732.tif\n",
      "733.tif\n",
      "734.tif\n",
      "735.tif\n",
      "736.tif\n",
      "737.tif\n",
      "738.tif\n",
      "739.tif\n",
      "740.tif\n",
      "741.tif\n",
      "742.tif\n",
      "743.tif\n",
      "744.tif\n",
      "745.tif\n",
      "746.tif\n",
      "747.tif\n",
      "748.tif\n",
      "749.tif\n",
      "750.tif\n",
      "751.tif\n",
      "752.tif\n",
      "753.tif\n",
      "754.tif\n",
      "755.tif\n",
      "756.tif\n",
      "757.tif\n",
      "758.tif\n",
      "759.tif\n",
      "760.tif\n",
      "761.tif\n",
      "762.tif\n",
      "763.tif\n",
      "764.tif\n",
      "765.tif\n",
      "766.tif\n",
      "767.tif\n",
      "768.tif\n",
      "769.tif\n",
      "770.tif\n",
      "771.tif\n",
      "772.tif\n",
      "773.tif\n",
      "774.tif\n",
      "775.tif\n",
      "776.tif\n",
      "777.tif\n",
      "778.tif\n",
      "779.tif\n",
      "780.tif\n",
      "781.tif\n",
      "782.tif\n",
      "783.tif\n",
      "784.tif\n",
      "785.tif\n",
      "786.tif\n",
      "787.tif\n",
      "788.tif\n",
      "789.tif\n",
      "790.tif\n",
      "791.tif\n",
      "792.tif\n",
      "793.tif\n",
      "794.tif\n",
      "795.tif\n",
      "796.tif\n",
      "797.tif\n",
      "798.tif\n",
      "799.tif\n",
      "800.tif\n",
      "801.tif\n",
      "802.tif\n",
      "803.tif\n",
      "804.tif\n",
      "805.tif\n",
      "806.tif\n",
      "807.tif\n",
      "808.tif\n",
      "809.tif\n",
      "810.tif\n",
      "811.tif\n",
      "812.tif\n",
      "813.tif\n",
      "814.tif\n",
      "815.tif\n",
      "816.tif\n",
      "817.tif\n",
      "818.tif\n",
      "819.tif\n",
      "820.tif\n",
      "821.tif\n",
      "822.tif\n",
      "823.tif\n",
      "824.tif\n",
      "825.tif\n",
      "826.tif\n",
      "827.tif\n",
      "828.tif\n",
      "829.tif\n",
      "830.tif\n",
      "831.tif\n",
      "832.tif\n",
      "0.tif\n",
      "1.tif\n",
      "2.tif\n",
      "3.tif\n",
      "4.tif\n",
      "5.tif\n",
      "6.tif\n",
      "7.tif\n",
      "8.tif\n",
      "9.tif\n",
      "10.tif\n",
      "11.tif\n",
      "12.tif\n",
      "13.tif\n",
      "14.tif\n",
      "15.tif\n",
      "16.tif\n",
      "17.tif\n",
      "18.tif\n",
      "19.tif\n",
      "20.tif\n",
      "21.tif\n",
      "22.tif\n",
      "23.tif\n",
      "24.tif\n",
      "25.tif\n",
      "26.tif\n",
      "27.tif\n",
      "28.tif\n",
      "29.tif\n",
      "30.tif\n",
      "31.tif\n",
      "32.tif\n",
      "33.tif\n",
      "34.tif\n",
      "35.tif\n",
      "36.tif\n",
      "37.tif\n",
      "38.tif\n",
      "39.tif\n",
      "40.tif\n",
      "41.tif\n",
      "42.tif\n",
      "43.tif\n",
      "44.tif\n",
      "45.tif\n",
      "46.tif\n",
      "47.tif\n",
      "48.tif\n",
      "49.tif\n",
      "50.tif\n",
      "51.tif\n",
      "52.tif\n",
      "53.tif\n",
      "54.tif\n",
      "55.tif\n",
      "56.tif\n",
      "57.tif\n",
      "58.tif\n",
      "59.tif\n",
      "60.tif\n",
      "61.tif\n",
      "62.tif\n",
      "63.tif\n",
      "64.tif\n",
      "65.tif\n",
      "66.tif\n",
      "67.tif\n",
      "68.tif\n",
      "69.tif\n",
      "70.tif\n",
      "71.tif\n",
      "72.tif\n",
      "73.tif\n",
      "74.tif\n",
      "75.tif\n",
      "76.tif\n",
      "77.tif\n",
      "78.tif\n",
      "79.tif\n",
      "80.tif\n",
      "81.tif\n",
      "82.tif\n",
      "83.tif\n",
      "84.tif\n",
      "85.tif\n",
      "86.tif\n",
      "87.tif\n",
      "88.tif\n",
      "89.tif\n",
      "90.tif\n",
      "91.tif\n",
      "92.tif\n",
      "93.tif\n",
      "94.tif\n",
      "95.tif\n",
      "96.tif\n",
      "97.tif\n",
      "98.tif\n",
      "99.tif\n",
      "100.tif\n",
      "101.tif\n",
      "102.tif\n",
      "103.tif\n",
      "104.tif\n",
      "105.tif\n",
      "106.tif\n",
      "107.tif\n",
      "108.tif\n",
      "109.tif\n",
      "110.tif\n",
      "111.tif\n",
      "112.tif\n",
      "113.tif\n",
      "114.tif\n",
      "115.tif\n",
      "116.tif\n",
      "117.tif\n",
      "118.tif\n",
      "119.tif\n",
      "120.tif\n",
      "121.tif\n",
      "122.tif\n",
      "123.tif\n",
      "124.tif\n",
      "125.tif\n",
      "126.tif\n",
      "127.tif\n",
      "128.tif\n",
      "129.tif\n",
      "130.tif\n",
      "131.tif\n",
      "132.tif\n",
      "133.tif\n",
      "134.tif\n",
      "135.tif\n",
      "136.tif\n",
      "137.tif\n",
      "138.tif\n",
      "139.tif\n",
      "140.tif\n",
      "141.tif\n",
      "142.tif\n",
      "143.tif\n",
      "144.tif\n",
      "145.tif\n",
      "146.tif\n",
      "147.tif\n",
      "148.tif\n",
      "149.tif\n",
      "150.tif\n",
      "151.tif\n",
      "152.tif\n",
      "153.tif\n",
      "154.tif\n",
      "155.tif\n",
      "156.tif\n",
      "157.tif\n",
      "158.tif\n",
      "159.tif\n",
      "160.tif\n",
      "161.tif\n",
      "162.tif\n",
      "163.tif\n",
      "164.tif\n",
      "165.tif\n",
      "166.tif\n",
      "167.tif\n",
      "168.tif\n",
      "169.tif\n",
      "170.tif\n",
      "171.tif\n",
      "172.tif\n",
      "173.tif\n",
      "174.tif\n",
      "175.tif\n",
      "176.tif\n",
      "177.tif\n",
      "178.tif\n",
      "179.tif\n",
      "180.tif\n",
      "181.tif\n",
      "182.tif\n",
      "183.tif\n",
      "184.tif\n",
      "185.tif\n",
      "186.tif\n",
      "187.tif\n",
      "188.tif\n",
      "189.tif\n",
      "190.tif\n",
      "191.tif\n",
      "192.tif\n",
      "193.tif\n",
      "194.tif\n",
      "195.tif\n",
      "196.tif\n",
      "197.tif\n",
      "198.tif\n",
      "199.tif\n",
      "200.tif\n",
      "201.tif\n",
      "202.tif\n",
      "203.tif\n",
      "204.tif\n",
      "205.tif\n",
      "206.tif\n",
      "207.tif\n",
      "208.tif\n",
      "209.tif\n",
      "210.tif\n",
      "211.tif\n",
      "212.tif\n",
      "213.tif\n",
      "214.tif\n",
      "215.tif\n",
      "216.tif\n",
      "217.tif\n",
      "218.tif\n",
      "219.tif\n",
      "220.tif\n",
      "221.tif\n",
      "222.tif\n",
      "223.tif\n",
      "224.tif\n",
      "225.tif\n",
      "226.tif\n",
      "227.tif\n",
      "228.tif\n",
      "229.tif\n",
      "230.tif\n",
      "231.tif\n",
      "232.tif\n",
      "233.tif\n",
      "234.tif\n",
      "235.tif\n",
      "236.tif\n",
      "237.tif\n",
      "238.tif\n",
      "239.tif\n",
      "240.tif\n",
      "241.tif\n",
      "242.tif\n",
      "243.tif\n",
      "244.tif\n",
      "245.tif\n",
      "246.tif\n",
      "247.tif\n",
      "248.tif\n",
      "249.tif\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250.tif\n",
      "251.tif\n",
      "252.tif\n",
      "253.tif\n",
      "254.tif\n",
      "255.tif\n",
      "256.tif\n",
      "257.tif\n",
      "258.tif\n",
      "259.tif\n",
      "260.tif\n",
      "261.tif\n",
      "262.tif\n",
      "263.tif\n",
      "264.tif\n",
      "265.tif\n",
      "266.tif\n",
      "267.tif\n",
      "268.tif\n",
      "269.tif\n",
      "270.tif\n",
      "271.tif\n",
      "272.tif\n",
      "273.tif\n",
      "274.tif\n",
      "275.tif\n",
      "276.tif\n",
      "277.tif\n",
      "278.tif\n",
      "279.tif\n",
      "280.tif\n",
      "281.tif\n",
      "282.tif\n",
      "283.tif\n",
      "284.tif\n",
      "285.tif\n",
      "286.tif\n",
      "287.tif\n",
      "288.tif\n",
      "289.tif\n",
      "290.tif\n",
      "291.tif\n",
      "292.tif\n",
      "293.tif\n",
      "294.tif\n",
      "295.tif\n",
      "296.tif\n",
      "297.tif\n",
      "298.tif\n",
      "299.tif\n",
      "300.tif\n",
      "301.tif\n",
      "302.tif\n",
      "303.tif\n",
      "304.tif\n",
      "305.tif\n",
      "306.tif\n",
      "307.tif\n",
      "308.tif\n",
      "309.tif\n",
      "310.tif\n",
      "311.tif\n",
      "312.tif\n",
      "313.tif\n",
      "314.tif\n",
      "315.tif\n",
      "316.tif\n",
      "317.tif\n",
      "318.tif\n",
      "319.tif\n",
      "320.tif\n",
      "321.tif\n",
      "322.tif\n",
      "323.tif\n",
      "324.tif\n",
      "325.tif\n",
      "326.tif\n",
      "327.tif\n",
      "328.tif\n",
      "329.tif\n",
      "330.tif\n",
      "331.tif\n",
      "332.tif\n",
      "333.tif\n",
      "334.tif\n",
      "335.tif\n",
      "336.tif\n",
      "337.tif\n",
      "338.tif\n",
      "339.tif\n",
      "340.tif\n",
      "341.tif\n",
      "342.tif\n",
      "343.tif\n",
      "344.tif\n",
      "345.tif\n",
      "346.tif\n",
      "347.tif\n",
      "348.tif\n",
      "349.tif\n",
      "350.tif\n",
      "351.tif\n",
      "352.tif\n",
      "353.tif\n",
      "354.tif\n",
      "355.tif\n",
      "356.tif\n",
      "357.tif\n",
      "358.tif\n",
      "359.tif\n",
      "360.tif\n",
      "361.tif\n",
      "362.tif\n",
      "363.tif\n",
      "364.tif\n",
      "365.tif\n",
      "366.tif\n",
      "367.tif\n",
      "368.tif\n",
      "369.tif\n",
      "370.tif\n",
      "371.tif\n",
      "372.tif\n",
      "373.tif\n",
      "374.tif\n",
      "375.tif\n",
      "376.tif\n",
      "377.tif\n",
      "378.tif\n",
      "379.tif\n",
      "380.tif\n",
      "381.tif\n",
      "382.tif\n",
      "383.tif\n",
      "384.tif\n",
      "385.tif\n",
      "386.tif\n",
      "387.tif\n",
      "388.tif\n",
      "389.tif\n",
      "390.tif\n",
      "391.tif\n",
      "392.tif\n",
      "393.tif\n",
      "394.tif\n",
      "395.tif\n",
      "396.tif\n",
      "397.tif\n",
      "398.tif\n",
      "399.tif\n",
      "400.tif\n",
      "401.tif\n",
      "402.tif\n",
      "403.tif\n",
      "404.tif\n",
      "405.tif\n",
      "406.tif\n",
      "407.tif\n",
      "408.tif\n",
      "409.tif\n",
      "410.tif\n",
      "411.tif\n",
      "412.tif\n",
      "413.tif\n",
      "414.tif\n",
      "415.tif\n",
      "416.tif\n",
      "417.tif\n",
      "418.tif\n",
      "419.tif\n",
      "420.tif\n",
      "421.tif\n",
      "422.tif\n",
      "423.tif\n",
      "424.tif\n",
      "425.tif\n",
      "426.tif\n",
      "427.tif\n",
      "428.tif\n",
      "429.tif\n",
      "430.tif\n",
      "431.tif\n",
      "432.tif\n",
      "433.tif\n",
      "434.tif\n",
      "435.tif\n",
      "436.tif\n",
      "437.tif\n",
      "438.tif\n",
      "439.tif\n",
      "440.tif\n",
      "441.tif\n",
      "442.tif\n",
      "443.tif\n",
      "444.tif\n",
      "445.tif\n",
      "446.tif\n",
      "447.tif\n",
      "448.tif\n",
      "449.tif\n",
      "450.tif\n",
      "451.tif\n",
      "452.tif\n",
      "453.tif\n",
      "454.tif\n",
      "455.tif\n",
      "456.tif\n",
      "457.tif\n",
      "458.tif\n",
      "459.tif\n",
      "460.tif\n",
      "461.tif\n",
      "462.tif\n",
      "463.tif\n",
      "464.tif\n",
      "465.tif\n",
      "466.tif\n",
      "467.tif\n",
      "468.tif\n",
      "469.tif\n",
      "470.tif\n",
      "471.tif\n",
      "472.tif\n",
      "473.tif\n",
      "474.tif\n",
      "475.tif\n",
      "476.tif\n",
      "477.tif\n",
      "478.tif\n",
      "479.tif\n",
      "480.tif\n",
      "481.tif\n",
      "482.tif\n",
      "483.tif\n",
      "484.tif\n",
      "485.tif\n",
      "486.tif\n",
      "487.tif\n",
      "488.tif\n",
      "489.tif\n",
      "490.tif\n",
      "491.tif\n",
      "492.tif\n",
      "493.tif\n",
      "494.tif\n",
      "495.tif\n",
      "496.tif\n",
      "497.tif\n",
      "498.tif\n",
      "499.tif\n",
      "500.tif\n",
      "501.tif\n",
      "502.tif\n",
      "503.tif\n",
      "504.tif\n",
      "505.tif\n",
      "506.tif\n",
      "507.tif\n",
      "508.tif\n",
      "509.tif\n",
      "510.tif\n",
      "511.tif\n",
      "512.tif\n",
      "513.tif\n",
      "514.tif\n",
      "515.tif\n",
      "516.tif\n",
      "517.tif\n",
      "518.tif\n",
      "519.tif\n",
      "520.tif\n",
      "521.tif\n",
      "522.tif\n",
      "523.tif\n",
      "524.tif\n",
      "525.tif\n",
      "526.tif\n",
      "527.tif\n",
      "528.tif\n",
      "529.tif\n",
      "530.tif\n",
      "531.tif\n",
      "532.tif\n",
      "533.tif\n",
      "534.tif\n",
      "535.tif\n",
      "536.tif\n",
      "537.tif\n",
      "538.tif\n",
      "539.tif\n",
      "540.tif\n",
      "541.tif\n",
      "542.tif\n",
      "543.tif\n",
      "544.tif\n",
      "545.tif\n",
      "546.tif\n",
      "547.tif\n",
      "548.tif\n",
      "549.tif\n",
      "550.tif\n",
      "551.tif\n",
      "552.tif\n",
      "553.tif\n",
      "554.tif\n",
      "555.tif\n",
      "556.tif\n",
      "557.tif\n",
      "558.tif\n",
      "559.tif\n",
      "560.tif\n",
      "561.tif\n",
      "562.tif\n",
      "563.tif\n",
      "564.tif\n",
      "565.tif\n",
      "566.tif\n",
      "567.tif\n",
      "568.tif\n",
      "569.tif\n",
      "570.tif\n",
      "571.tif\n",
      "572.tif\n",
      "573.tif\n",
      "574.tif\n",
      "575.tif\n",
      "576.tif\n",
      "577.tif\n",
      "578.tif\n",
      "579.tif\n",
      "580.tif\n",
      "581.tif\n",
      "582.tif\n",
      "583.tif\n",
      "584.tif\n",
      "585.tif\n",
      "586.tif\n",
      "587.tif\n",
      "588.tif\n",
      "589.tif\n",
      "590.tif\n",
      "591.tif\n",
      "592.tif\n",
      "593.tif\n",
      "594.tif\n",
      "595.tif\n",
      "596.tif\n",
      "597.tif\n",
      "598.tif\n",
      "599.tif\n",
      "600.tif\n",
      "601.tif\n",
      "602.tif\n",
      "603.tif\n",
      "604.tif\n",
      "605.tif\n",
      "606.tif\n",
      "607.tif\n",
      "608.tif\n",
      "609.tif\n",
      "610.tif\n",
      "611.tif\n",
      "612.tif\n",
      "613.tif\n",
      "614.tif\n",
      "615.tif\n",
      "616.tif\n",
      "617.tif\n",
      "618.tif\n",
      "619.tif\n",
      "620.tif\n",
      "621.tif\n",
      "622.tif\n",
      "623.tif\n",
      "624.tif\n",
      "625.tif\n",
      "626.tif\n",
      "627.tif\n",
      "628.tif\n",
      "629.tif\n",
      "630.tif\n",
      "631.tif\n",
      "632.tif\n",
      "633.tif\n",
      "634.tif\n",
      "635.tif\n",
      "636.tif\n",
      "637.tif\n",
      "638.tif\n",
      "639.tif\n",
      "640.tif\n",
      "641.tif\n",
      "642.tif\n",
      "643.tif\n",
      "644.tif\n",
      "645.tif\n",
      "646.tif\n",
      "647.tif\n",
      "648.tif\n",
      "649.tif\n",
      "650.tif\n",
      "651.tif\n",
      "652.tif\n",
      "653.tif\n",
      "654.tif\n",
      "655.tif\n",
      "656.tif\n",
      "657.tif\n",
      "658.tif\n",
      "659.tif\n",
      "660.tif\n",
      "661.tif\n",
      "662.tif\n",
      "663.tif\n",
      "664.tif\n",
      "665.tif\n",
      "666.tif\n",
      "667.tif\n",
      "668.tif\n",
      "669.tif\n",
      "670.tif\n",
      "671.tif\n",
      "672.tif\n",
      "673.tif\n",
      "674.tif\n",
      "675.tif\n",
      "676.tif\n",
      "677.tif\n",
      "678.tif\n",
      "679.tif\n",
      "680.tif\n",
      "681.tif\n",
      "682.tif\n",
      "683.tif\n",
      "684.tif\n",
      "685.tif\n",
      "686.tif\n",
      "687.tif\n",
      "688.tif\n",
      "689.tif\n",
      "690.tif\n",
      "691.tif\n",
      "692.tif\n",
      "693.tif\n",
      "694.tif\n",
      "695.tif\n",
      "696.tif\n",
      "697.tif\n",
      "698.tif\n",
      "699.tif\n",
      "700.tif\n",
      "701.tif\n",
      "702.tif\n",
      "703.tif\n",
      "704.tif\n",
      "705.tif\n",
      "706.tif\n",
      "707.tif\n",
      "708.tif\n",
      "709.tif\n",
      "710.tif\n",
      "711.tif\n",
      "712.tif\n",
      "713.tif\n",
      "714.tif\n",
      "715.tif\n",
      "716.tif\n",
      "717.tif\n",
      "718.tif\n",
      "719.tif\n",
      "720.tif\n",
      "721.tif\n",
      "722.tif\n",
      "723.tif\n",
      "724.tif\n",
      "725.tif\n",
      "726.tif\n",
      "727.tif\n",
      "728.tif\n",
      "729.tif\n",
      "730.tif\n",
      "731.tif\n",
      "732.tif\n",
      "733.tif\n",
      "734.tif\n",
      "735.tif\n",
      "736.tif\n",
      "737.tif\n",
      "738.tif\n",
      "739.tif\n",
      "740.tif\n",
      "741.tif\n",
      "742.tif\n",
      "743.tif\n",
      "744.tif\n",
      "745.tif\n",
      "746.tif\n",
      "747.tif\n",
      "748.tif\n",
      "749.tif\n",
      "750.tif\n",
      "751.tif\n",
      "752.tif\n",
      "753.tif\n",
      "754.tif\n",
      "755.tif\n",
      "756.tif\n",
      "757.tif\n",
      "758.tif\n",
      "759.tif\n",
      "760.tif\n",
      "761.tif\n",
      "762.tif\n",
      "763.tif\n",
      "764.tif\n",
      "765.tif\n",
      "766.tif\n",
      "767.tif\n",
      "768.tif\n",
      "769.tif\n",
      "770.tif\n",
      "771.tif\n",
      "772.tif\n",
      "773.tif\n",
      "774.tif\n",
      "775.tif\n",
      "776.tif\n",
      "777.tif\n",
      "778.tif\n",
      "779.tif\n",
      "780.tif\n",
      "781.tif\n",
      "782.tif\n",
      "783.tif\n",
      "784.tif\n",
      "785.tif\n",
      "786.tif\n",
      "787.tif\n",
      "788.tif\n",
      "789.tif\n",
      "790.tif\n",
      "791.tif\n",
      "792.tif\n",
      "793.tif\n",
      "794.tif\n",
      "795.tif\n",
      "796.tif\n",
      "797.tif\n",
      "798.tif\n",
      "799.tif\n",
      "800.tif\n",
      "801.tif\n",
      "802.tif\n",
      "803.tif\n",
      "804.tif\n",
      "805.tif\n",
      "806.tif\n",
      "807.tif\n",
      "808.tif\n",
      "809.tif\n",
      "810.tif\n",
      "811.tif\n",
      "812.tif\n",
      "813.tif\n",
      "814.tif\n",
      "815.tif\n",
      "816.tif\n",
      "817.tif\n",
      "818.tif\n",
      "819.tif\n",
      "820.tif\n",
      "821.tif\n",
      "822.tif\n",
      "823.tif\n",
      "824.tif\n",
      "825.tif\n",
      "826.tif\n",
      "827.tif\n",
      "828.tif\n",
      "829.tif\n",
      "830.tif\n",
      "831.tif\n",
      "832.tif\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(833, 128, 256, 1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def atoi(text) : \n",
    "    return int(text) if text.isdigit() else text\n",
    "def natural_keys(text) :\n",
    "    return [atoi(c) for c in re.split('(\\d+)', text)]\n",
    "\n",
    "\n",
    "root_path = \"\"\n",
    "filenames = []\n",
    "\n",
    "for root, dirnames, filenames in os.walk(\"./data/oct_gt_resized/\"):\n",
    "    filenames.sort(key = natural_keys)\n",
    "    rootpath = root\n",
    "ground_truth_rs = []\n",
    "for filename in filenames :\n",
    "    filepath = os.path.join(root,filename)\n",
    "    image = ndimage.imread(filepath, mode = \"L\")\n",
    "    ground_truth_rs.append(image)\n",
    "    print(filename)\n",
    "\n",
    "\n",
    "root_path = \"\"\n",
    "filenames = []\n",
    "\n",
    "for root, dirnames, filenames in os.walk(\"./data/oct_noisy_resized/\"):\n",
    "    filenames.sort(key = natural_keys)\n",
    "    rootpath = root\n",
    "noisy_images_rs = []\n",
    "for filename in filenames :\n",
    "    filepath = os.path.join(root,filename)\n",
    "    image = ndimage.imread(filepath, mode = \"L\")\n",
    "    noisy_images_rs.append(image)\n",
    "    print(filename)\n",
    "\n",
    "   \n",
    "ground_truth_rs = np.array(ground_truth_rs)\n",
    "ground_truth_rs.shape\n",
    "ground_truth_rs = np.resize(ground_truth_rs,(ground_truth_rs.shape[0],ground_truth_rs.shape[1],ground_truth_rs.shape[2],1))\n",
    "ground_truth_rs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resize_images(src_path,dest_path,sizeX,sizeY):\n",
    "    images = []\n",
    "    width,height = sizeX,sizeY\n",
    "    for filename in os.listdir(src_path):\n",
    "        img = cv2.imread(src_path + filename)\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    cnt =1\n",
    "    for img in images:\n",
    "        im2 = cv2.resize(img,(width, height),interpolation = cv2.INTER_CUBIC)\n",
    "        \n",
    "        \n",
    "        ext = \".png\"\n",
    "        cv2.imwrite(dest_path + \"image\" +  str(cnt) + ext,im2)\n",
    "        cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_images = read_images_from_folder('./data/oct_gt/')\n",
    "raw_images = np.resize(raw_images,(raw_images.shape[0],448,900,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adding noise to images\n",
    "def add_noise(images):\n",
    "    \n",
    "    noisy_set = []\n",
    "    for image in images:\n",
    "        print len(noisy_set)\n",
    "        for shape in xrange(2,9,1):\n",
    "            for scale in xrange(2,9,1):\n",
    "                noise = np.random.gamma(shape,scale,image.shape)\n",
    "                noisy_set.append((image,np.clip(noise + image,0.,255.)))\n",
    "    shuffle(noisy_set)\n",
    "    return np.array(noisy_set)\n",
    "   \n",
    "#Shuffle the noisy image ground truth pair to randomize the noise distribution in the dataset\n",
    "def expand_pair(noisy_set):   \n",
    "    ground_truth=[]\n",
    "    noisy_images = []\n",
    "    for i in range(noisy_set.shape[0]):\n",
    "        ground_truth.append(noisy_set[i][0].reshape((noisy_set[i][0].shape[0],noisy_set[i][0].shape[1],1)))\n",
    "        #print( str(noisy_set[i][0].shape[0]) +\" \"+ str(noisy_set[i][0].shape[1]))\n",
    "        noisy_images.append(noisy_set[i][1].reshape((noisy_set[i][1].shape[0],noisy_set[i][1].shape[1],1)))\n",
    "    return np.array(ground_truth), np.array(noisy_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "49\n",
      "98\n",
      "147\n",
      "196\n",
      "245\n",
      "294\n",
      "343\n",
      "392\n",
      "441\n",
      "490\n",
      "539\n",
      "588\n",
      "637\n",
      "686\n",
      "735\n",
      "784\n"
     ]
    }
   ],
   "source": [
    "noisy_set = add_noise(raw_images)\n",
    "for i in range(noisy_set.shape[0]):\n",
    "    cv2.imwrite(\"./data/oct_noisy/\"+str(i)+\".tif\",noisy_set[i][1])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((833, 448, 900, 1), (833, 448, 900, 1), (833, 2, 448, 900, 1))\n"
     ]
    }
   ],
   "source": [
    "#Shuffling and adding noise to the dataset\n",
    "ground_truth,noisy_images = expand_pair(noisy_set)\n",
    "print (ground_truth.shape, noisy_images.shape, noisy_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.125008434593\n"
     ]
    }
   ],
   "source": [
    "print np.std(ground_truth/255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images(images,size):\n",
    "    resized_images = []\n",
    "    \n",
    "    for img in images:\n",
    "        im2 = cv2.resize(img,size,interpolation = cv2.INTER_CUBIC)\n",
    "        resized_images.append(im2)\n",
    "    \n",
    "    resized_images = np.array(resized_images)\n",
    "    resized_images = np.reshape(resized_images,(resized_images.shape[0],resized_images.shape[1],resized_images.shape[2],1))\n",
    "    \n",
    "    return resized_images\n",
    "\n",
    "ground_truth_rs = resize_images(ground_truth,(256,128))\n",
    "noisy_images_rs = resize_images(noisy_images,(256,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(833, 128, 256, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_images_rs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(noisy_images_rs.shape[0]):\n",
    "    cv2.imwrite(\"./data/oct_noisy_resized/\"+str(i)+\".tif\",noisy_images_rs[i])  \n",
    "    \n",
    "for i in range(ground_truth_rs.shape[0]):\n",
    "    cv2.imwrite(\"./data/oct_gt_resized/\"+str(i)+\".tif\",ground_truth_rs[i])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_rs = read_images_from_folder('./data/oct_gt_resized/')\n",
    "ground_truth_rs = np.resize(ground_truth_rs,(ground_truth_rs.shape[0],ground_truth_rs.shape[1],ground_truth_rs.shape[2],1))\n",
    "\n",
    "noisy_images_rs = read_images_from_folder('./data/oct_noisy_resized/')\n",
    "noisy_images_rs = np.resize(noisy_images_rs,(noisy_images_rs.shape[0],noisy_images_rs.shape[1],noisy_images_rs.shape[2],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('test_noisy.png',noisy_images_rs[33])\n",
    "cv2.imwrite('test.png',ground_truth_rs[33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(666, 128, 256, 1)\n",
      "(167, 128, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "#Split into training and cross validation and normalizing\n",
    "train_size = int(ground_truth_rs.shape[0]*0.8)\n",
    "x_train = ground_truth_rs[0:train_size]/255.\n",
    "x_train_noisy = noisy_images_rs[0:train_size]/255.\n",
    "x_test = ground_truth_rs[train_size:]/255.\n",
    "x_test_noisy = noisy_images_rs[train_size:]/255.\n",
    "print (x_train_noisy.shape)\n",
    "print (x_test_noisy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lr_reducer = ReduceLROnPlateau(factor = np.sqrt(0.1), cooldown=0, patience=2, min_lr=0.5e-6)\n",
    "#csv_logger = CSVLogger('./models/simple_cnn_autoencoder.csv')\n",
    "#early_stopper = EarlyStopping(min_delta=0.001,patience=30)\n",
    "model_checkpoint = ModelCheckpoint('./models/simple_cnn_oct.hdf5',monitor = 'loss', verbose = 1,save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(666, 128, 256, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_noisy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining the model\n",
    "\n",
    "def get_simple_cnn_autoencoder_model(model_path=None):\n",
    "    \n",
    "    if(model_path is None):\n",
    "        autoencoder = None\n",
    "    else:\n",
    "        autoencoder = read_model_json(model_path) \n",
    "    \n",
    "    if(autoencoder is None):\n",
    "        input_img = Input(shape=((None,None,1)))  # adapt this if using `channels_first` image data format\n",
    "\n",
    "        x = Conv2D(64, (3, 3), activation='relu', padding='same')(input_img)\n",
    "        x = AveragePooling2D((2, 2), padding='same')(x)\n",
    "        x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "        encoded = AveragePooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "        x = Conv2D(64, (3, 3), activation='relu', padding='same')(encoded)\n",
    "        x = UpSampling2D((2, 2))(x)\n",
    "        x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = UpSampling2D((2, 2))(x)\n",
    "        \n",
    "        decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "        autoencoder = Model(input_img, decoded)\n",
    "        \n",
    "        \n",
    "        autoencoder.compile(optimizer='Adam', loss='binary_crossentropy',metrics = ['accuracy','mean_squared_error'])\n",
    "\n",
    "    print (autoencoder.summary())\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, None, None, 1)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, None, None, 64)    640       \n",
      "_________________________________________________________________\n",
      "average_pooling2d_5 (Average (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_6 (Average (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_9 (UpSampling2 (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_10 (UpSampling (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, None, None, 1)     577       \n",
      "=================================================================\n",
      "Total params: 112,001\n",
      "Trainable params: 112,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Training the model\n",
    "autoencoder = get_simple_cnn_autoencoder_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_model(autoencoder, to_file='model.png',show_shapes='True',show_layer_names='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 666 samples, validate on 167 samples\n",
      "Epoch 1/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6270 - acc: 0.0000e+00 - mean_squared_error: 0.0121Epoch 00000: loss improved from inf to 0.62671, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 53s - loss: 0.6267 - acc: 0.0000e+00 - mean_squared_error: 0.0120 - val_loss: 0.6170 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0063\n",
      "Epoch 2/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6117 - acc: 0.0000e+00 - mean_squared_error: 0.0052Epoch 00001: loss improved from 0.62671 to 0.61169, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6117 - acc: 0.0000e+00 - mean_squared_error: 0.0052 - val_loss: 0.6140 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0049\n",
      "Epoch 3/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6090 - acc: 0.0000e+00 - mean_squared_error: 0.0039Epoch 00002: loss improved from 0.61169 to 0.60904, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6090 - acc: 0.0000e+00 - mean_squared_error: 0.0039 - val_loss: 0.6176 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0065\n",
      "Epoch 4/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6071 - acc: 0.0000e+00 - mean_squared_error: 0.0030Epoch 00003: loss improved from 0.60904 to 0.60715, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6071 - acc: 0.0000e+00 - mean_squared_error: 0.0030 - val_loss: 0.6149 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0053\n",
      "Epoch 5/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6061 - acc: 0.0000e+00 - mean_squared_error: 0.0026Epoch 00004: loss improved from 0.60715 to 0.60609, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6061 - acc: 0.0000e+00 - mean_squared_error: 0.0026 - val_loss: 0.6094 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0028\n",
      "Epoch 6/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6059 - acc: 0.0000e+00 - mean_squared_error: 0.0025Epoch 00005: loss improved from 0.60609 to 0.60592, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6059 - acc: 0.0000e+00 - mean_squared_error: 0.0025 - val_loss: 0.6085 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0023\n",
      "Epoch 7/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6047 - acc: 0.0000e+00 - mean_squared_error: 0.0020Epoch 00006: loss improved from 0.60592 to 0.60481, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6048 - acc: 0.0000e+00 - mean_squared_error: 0.0020 - val_loss: 0.6089 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0025\n",
      "Epoch 8/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6052 - acc: 0.0000e+00 - mean_squared_error: 0.0022Epoch 00007: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6053 - acc: 0.0000e+00 - mean_squared_error: 0.0022 - val_loss: 0.6074 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0019\n",
      "Epoch 9/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6042 - acc: 0.0000e+00 - mean_squared_error: 0.0017Epoch 00008: loss improved from 0.60481 to 0.60418, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6042 - acc: 0.0000e+00 - mean_squared_error: 0.0017 - val_loss: 0.6070 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0017\n",
      "Epoch 10/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6038 - acc: 0.0000e+00 - mean_squared_error: 0.0016Epoch 00009: loss improved from 0.60418 to 0.60384, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6038 - acc: 0.0000e+00 - mean_squared_error: 0.0016 - val_loss: 0.6068 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0016\n",
      "Epoch 11/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6040 - acc: 0.0000e+00 - mean_squared_error: 0.0016Epoch 00010: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6040 - acc: 0.0000e+00 - mean_squared_error: 0.0016 - val_loss: 0.6066 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0015\n",
      "Epoch 12/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6042 - acc: 0.0000e+00 - mean_squared_error: 0.0017Epoch 00011: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6041 - acc: 0.0000e+00 - mean_squared_error: 0.0017 - val_loss: 0.6072 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0018\n",
      "Epoch 13/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6039 - acc: 0.0000e+00 - mean_squared_error: 0.0016Epoch 00012: loss improved from 0.60384 to 0.60383, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6038 - acc: 0.0000e+00 - mean_squared_error: 0.0016 - val_loss: 0.6092 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0026\n",
      "Epoch 14/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6039 - acc: 0.0000e+00 - mean_squared_error: 0.0016Epoch 00013: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6039 - acc: 0.0000e+00 - mean_squared_error: 0.0016 - val_loss: 0.6064 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0014\n",
      "Epoch 15/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6034 - acc: 0.0000e+00 - mean_squared_error: 0.0014Epoch 00014: loss improved from 0.60383 to 0.60344, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6034 - acc: 0.0000e+00 - mean_squared_error: 0.0014 - val_loss: 0.6077 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0020\n",
      "Epoch 16/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6035 - acc: 0.0000e+00 - mean_squared_error: 0.0014Epoch 00015: loss improved from 0.60344 to 0.60342, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6034 - acc: 0.0000e+00 - mean_squared_error: 0.0014 - val_loss: 0.6065 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0015\n",
      "Epoch 17/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6036 - acc: 0.0000e+00 - mean_squared_error: 0.0014Epoch 00016: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6035 - acc: 0.0000e+00 - mean_squared_error: 0.0014 - val_loss: 0.6067 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0016\n",
      "Epoch 18/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6036 - acc: 0.0000e+00 - mean_squared_error: 0.0014Epoch 00017: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6035 - acc: 0.0000e+00 - mean_squared_error: 0.0014 - val_loss: 0.6063 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0014\n",
      "Epoch 19/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6033 - acc: 0.0000e+00 - mean_squared_error: 0.0013Epoch 00018: loss improved from 0.60342 to 0.60329, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6033 - acc: 0.0000e+00 - mean_squared_error: 0.0013 - val_loss: 0.6063 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0014\n",
      "Epoch 20/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6031 - acc: 0.0000e+00 - mean_squared_error: 0.0013Epoch 00019: loss improved from 0.60329 to 0.60317, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6032 - acc: 0.0000e+00 - mean_squared_error: 0.0013 - val_loss: 0.6063 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0014\n",
      "Epoch 21/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6032 - acc: 0.0000e+00 - mean_squared_error: 0.0013Epoch 00020: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6033 - acc: 0.0000e+00 - mean_squared_error: 0.0013 - val_loss: 0.6070 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0017\n",
      "Epoch 22/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6033 - acc: 0.0000e+00 - mean_squared_error: 0.0014Epoch 00021: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6033 - acc: 0.0000e+00 - mean_squared_error: 0.0014 - val_loss: 0.6075 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0019\n",
      "Epoch 23/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6035 - acc: 0.0000e+00 - mean_squared_error: 0.0014Epoch 00022: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6035 - acc: 0.0000e+00 - mean_squared_error: 0.0014 - val_loss: 0.6066 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0015\n",
      "Epoch 24/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6034 - acc: 0.0000e+00 - mean_squared_error: 0.0014Epoch 00023: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6033 - acc: 0.0000e+00 - mean_squared_error: 0.0014 - val_loss: 0.6067 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0015\n",
      "Epoch 25/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6033 - acc: 0.0000e+00 - mean_squared_error: 0.0013Epoch 00024: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6032 - acc: 0.0000e+00 - mean_squared_error: 0.0013 - val_loss: 0.6060 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0013\n",
      "Epoch 26/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6030 - acc: 0.0000e+00 - mean_squared_error: 0.0012Epoch 00025: loss improved from 0.60317 to 0.60305, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6030 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - val_loss: 0.6060 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012\n",
      "Epoch 27/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6034 - acc: 0.0000e+00 - mean_squared_error: 0.0013Epoch 00026: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6033 - acc: 0.0000e+00 - mean_squared_error: 0.0013 - val_loss: 0.6061 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0013\n",
      "Epoch 28/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6034 - acc: 0.0000e+00 - mean_squared_error: 0.0013Epoch 00027: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6033 - acc: 0.0000e+00 - mean_squared_error: 0.0013 - val_loss: 0.6067 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0015\n",
      "Epoch 29/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6030 - acc: 0.0000e+00 - mean_squared_error: 0.0013Epoch 00028: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6031 - acc: 0.0000e+00 - mean_squared_error: 0.0013 - val_loss: 0.6065 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0015\n",
      "Epoch 30/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6032 - acc: 0.0000e+00 - mean_squared_error: 0.0013Epoch 00029: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6033 - acc: 0.0000e+00 - mean_squared_error: 0.0013 - val_loss: 0.6071 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0017\n",
      "Epoch 31/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6029 - acc: 0.0000e+00 - mean_squared_error: 0.0012Epoch 00030: loss improved from 0.60305 to 0.60301, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6030 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - val_loss: 0.6059 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012\n",
      "Epoch 32/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6030 - acc: 0.0000e+00 - mean_squared_error: 0.0013Epoch 00031: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6031 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - val_loss: 0.6061 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0013\n",
      "Epoch 33/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6032 - acc: 0.0000e+00 - mean_squared_error: 0.0013Epoch 00032: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6032 - acc: 0.0000e+00 - mean_squared_error: 0.0013 - val_loss: 0.6061 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0013\n",
      "Epoch 34/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6031 - acc: 0.0000e+00 - mean_squared_error: 0.0012Epoch 00033: loss improved from 0.60301 to 0.60299, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6030 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - val_loss: 0.6059 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012\n",
      "Epoch 35/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6033 - acc: 0.0000e+00 - mean_squared_error: 0.0014Epoch 00034: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6033 - acc: 0.0000e+00 - mean_squared_error: 0.0014 - val_loss: 0.6063 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0014\n",
      "Epoch 36/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6031 - acc: 0.0000e+00 - mean_squared_error: 0.0012Epoch 00035: loss improved from 0.60299 to 0.60292, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6029 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - val_loss: 0.6070 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0017\n",
      "Epoch 37/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6032 - acc: 0.0000e+00 - mean_squared_error: 0.0013Epoch 00036: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6033 - acc: 0.0000e+00 - mean_squared_error: 0.0013 - val_loss: 0.6061 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0013\n",
      "Epoch 38/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6033 - acc: 0.0000e+00 - mean_squared_error: 0.0014Epoch 00037: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6034 - acc: 0.0000e+00 - mean_squared_error: 0.0014 - val_loss: 0.6061 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0013\n",
      "Epoch 39/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6032 - acc: 0.0000e+00 - mean_squared_error: 0.0012Epoch 00038: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6029 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - val_loss: 0.6063 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0014\n",
      "Epoch 40/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6030 - acc: 0.0000e+00 - mean_squared_error: 0.0012Epoch 00039: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6030 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - val_loss: 0.6066 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0015\n",
      "Epoch 41/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6029 - acc: 0.0000e+00 - mean_squared_error: 0.0012Epoch 00040: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6030 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - val_loss: 0.6058 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012\n",
      "Epoch 42/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6030 - acc: 0.0000e+00 - mean_squared_error: 0.0012Epoch 00041: loss improved from 0.60292 to 0.60291, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6029 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - val_loss: 0.6060 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0013\n",
      "Epoch 43/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6030 - acc: 0.0000e+00 - mean_squared_error: 0.0012Epoch 00042: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6030 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - val_loss: 0.6058 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012\n",
      "Epoch 44/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6027 - acc: 0.0000e+00 - mean_squared_error: 0.0011Epoch 00043: loss improved from 0.60291 to 0.60283, saving model to ./models/simple_cnn_oct.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "666/666 [==============================] - 48s - loss: 0.6028 - acc: 0.0000e+00 - mean_squared_error: 0.0011 - val_loss: 0.6058 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0011\n",
      "Epoch 45/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6029 - acc: 0.0000e+00 - mean_squared_error: 0.0012Epoch 00044: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6029 - acc: 0.0000e+00 - mean_squared_error: 0.0011 - val_loss: 0.6060 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012\n",
      "Epoch 46/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6030 - acc: 0.0000e+00 - mean_squared_error: 0.0012Epoch 00045: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6030 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - val_loss: 0.6058 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012\n",
      "Epoch 47/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6031 - acc: 0.0000e+00 - mean_squared_error: 0.0012Epoch 00046: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6030 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - val_loss: 0.6058 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0011\n",
      "Epoch 48/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6030 - acc: 0.0000e+00 - mean_squared_error: 0.0012Epoch 00047: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6031 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - val_loss: 0.6063 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0014\n",
      "Epoch 49/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6028 - acc: 0.0000e+00 - mean_squared_error: 0.0011Epoch 00048: loss improved from 0.60283 to 0.60281, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6028 - acc: 0.0000e+00 - mean_squared_error: 0.0011 - val_loss: 0.6059 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012\n",
      "Epoch 50/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6028 - acc: 0.0000e+00 - mean_squared_error: 0.0011Epoch 00049: loss improved from 0.60281 to 0.60276, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6028 - acc: 0.0000e+00 - mean_squared_error: 0.0011 - val_loss: 0.6057 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0011\n",
      "Epoch 51/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6028 - acc: 0.0000e+00 - mean_squared_error: 0.0011Epoch 00050: loss improved from 0.60276 to 0.60267, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6027 - acc: 0.0000e+00 - mean_squared_error: 0.0011 - val_loss: 0.6056 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0011\n",
      "Epoch 52/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6029 - acc: 0.0000e+00 - mean_squared_error: 0.0012Epoch 00051: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6029 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - val_loss: 0.6061 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0013\n",
      "Epoch 53/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6025 - acc: 0.0000e+00 - mean_squared_error: 0.0010Epoch 00052: loss improved from 0.60267 to 0.60261, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6026 - acc: 0.0000e+00 - mean_squared_error: 0.0010 - val_loss: 0.6078 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0020\n",
      "Epoch 54/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6031 - acc: 0.0000e+00 - mean_squared_error: 0.0012Epoch 00053: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6029 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - val_loss: 0.6066 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0015\n",
      "Epoch 55/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6031 - acc: 0.0000e+00 - mean_squared_error: 0.0013Epoch 00054: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6031 - acc: 0.0000e+00 - mean_squared_error: 0.0013 - val_loss: 0.6056 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0010\n",
      "Epoch 56/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6027 - acc: 0.0000e+00 - mean_squared_error: 0.0010Epoch 00055: loss improved from 0.60261 to 0.60260, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6026 - acc: 0.0000e+00 - mean_squared_error: 0.0010 - val_loss: 0.6056 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0011\n",
      "Epoch 57/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6027 - acc: 0.0000e+00 - mean_squared_error: 0.0010Epoch 00056: loss improved from 0.60260 to 0.60259, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6026 - acc: 0.0000e+00 - mean_squared_error: 0.0010 - val_loss: 0.6062 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0013\n",
      "Epoch 58/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6025 - acc: 0.0000e+00 - mean_squared_error: 0.0010Epoch 00057: loss improved from 0.60259 to 0.60256, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6026 - acc: 0.0000e+00 - mean_squared_error: 0.0010 - val_loss: 0.6062 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0013\n",
      "Epoch 59/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6026 - acc: 0.0000e+00 - mean_squared_error: 9.8107e-04Epoch 00058: loss improved from 0.60256 to 0.60249, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6025 - acc: 0.0000e+00 - mean_squared_error: 9.8003e-04 - val_loss: 0.6054 - val_acc: 0.0000e+00 - val_mean_squared_error: 9.6896e-04\n",
      "Epoch 60/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6024 - acc: 0.0000e+00 - mean_squared_error: 9.5018e-04Epoch 00059: loss improved from 0.60249 to 0.60243, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6024 - acc: 0.0000e+00 - mean_squared_error: 9.5079e-04 - val_loss: 0.6054 - val_acc: 0.0000e+00 - val_mean_squared_error: 9.5768e-04\n",
      "Epoch 61/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6023 - acc: 0.0000e+00 - mean_squared_error: 9.3296e-04Epoch 00060: loss improved from 0.60243 to 0.60239, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6024 - acc: 0.0000e+00 - mean_squared_error: 9.3494e-04 - val_loss: 0.6054 - val_acc: 0.0000e+00 - val_mean_squared_error: 9.5933e-04\n",
      "Epoch 62/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6025 - acc: 0.0000e+00 - mean_squared_error: 9.9425e-04Epoch 00061: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6025 - acc: 0.0000e+00 - mean_squared_error: 9.9806e-04 - val_loss: 0.6059 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012\n",
      "Epoch 63/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6027 - acc: 0.0000e+00 - mean_squared_error: 0.0011Epoch 00062: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6028 - acc: 0.0000e+00 - mean_squared_error: 0.0011 - val_loss: 0.6058 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012\n",
      "Epoch 64/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6023 - acc: 0.0000e+00 - mean_squared_error: 9.3125e-04Epoch 00063: loss improved from 0.60239 to 0.60239, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6024 - acc: 0.0000e+00 - mean_squared_error: 9.3223e-04 - val_loss: 0.6053 - val_acc: 0.0000e+00 - val_mean_squared_error: 9.3539e-04\n",
      "Epoch 65/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6023 - acc: 0.0000e+00 - mean_squared_error: 9.1273e-04Epoch 00064: loss improved from 0.60239 to 0.60235, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6023 - acc: 0.0000e+00 - mean_squared_error: 9.1362e-04 - val_loss: 0.6053 - val_acc: 0.0000e+00 - val_mean_squared_error: 9.1980e-04\n",
      "Epoch 66/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "660/666 [============================>.] - ETA: 0s - loss: 0.6024 - acc: 0.0000e+00 - mean_squared_error: 9.7207e-04Epoch 00065: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6025 - acc: 0.0000e+00 - mean_squared_error: 9.7271e-04 - val_loss: 0.6055 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0010\n",
      "Epoch 67/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6028 - acc: 0.0000e+00 - mean_squared_error: 0.0011Epoch 00066: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6027 - acc: 0.0000e+00 - mean_squared_error: 0.0011 - val_loss: 0.6054 - val_acc: 0.0000e+00 - val_mean_squared_error: 9.6456e-04\n",
      "Epoch 68/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6023 - acc: 0.0000e+00 - mean_squared_error: 9.2138e-04Epoch 00067: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6024 - acc: 0.0000e+00 - mean_squared_error: 9.2018e-04 - val_loss: 0.6056 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0010\n",
      "Epoch 69/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6023 - acc: 0.0000e+00 - mean_squared_error: 9.4779e-04Epoch 00068: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6024 - acc: 0.0000e+00 - mean_squared_error: 9.4691e-04 - val_loss: 0.6058 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0011\n",
      "Epoch 70/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6024 - acc: 0.0000e+00 - mean_squared_error: 9.6513e-04Epoch 00069: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6025 - acc: 0.0000e+00 - mean_squared_error: 9.6363e-04 - val_loss: 0.6055 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0010\n",
      "Epoch 71/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6027 - acc: 0.0000e+00 - mean_squared_error: 9.5711e-04Epoch 00070: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6025 - acc: 0.0000e+00 - mean_squared_error: 9.6318e-04 - val_loss: 0.6052 - val_acc: 0.0000e+00 - val_mean_squared_error: 9.0111e-04\n",
      "Epoch 72/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6024 - acc: 0.0000e+00 - mean_squared_error: 9.5325e-04Epoch 00071: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6024 - acc: 0.0000e+00 - mean_squared_error: 9.5296e-04 - val_loss: 0.6052 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.8695e-04\n",
      "Epoch 73/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6022 - acc: 0.0000e+00 - mean_squared_error: 8.6671e-04Epoch 00072: loss improved from 0.60235 to 0.60224, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6022 - acc: 0.0000e+00 - mean_squared_error: 8.6658e-04 - val_loss: 0.6052 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.8735e-04\n",
      "Epoch 74/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6023 - acc: 0.0000e+00 - mean_squared_error: 9.2904e-04Epoch 00073: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6024 - acc: 0.0000e+00 - mean_squared_error: 9.2961e-04 - val_loss: 0.6052 - val_acc: 0.0000e+00 - val_mean_squared_error: 9.0844e-04\n",
      "Epoch 75/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6022 - acc: 0.0000e+00 - mean_squared_error: 8.9158e-04Epoch 00074: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6023 - acc: 0.0000e+00 - mean_squared_error: 8.9037e-04 - val_loss: 0.6052 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.6419e-04\n",
      "Epoch 76/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6023 - acc: 0.0000e+00 - mean_squared_error: 8.6605e-04Epoch 00075: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6022 - acc: 0.0000e+00 - mean_squared_error: 8.6836e-04 - val_loss: 0.6052 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.7540e-04\n",
      "Epoch 77/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6024 - acc: 0.0000e+00 - mean_squared_error: 8.9558e-04Epoch 00076: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6023 - acc: 0.0000e+00 - mean_squared_error: 8.9748e-04 - val_loss: 0.6055 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0010\n",
      "Epoch 78/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6024 - acc: 0.0000e+00 - mean_squared_error: 9.5776e-04Epoch 00077: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6024 - acc: 0.0000e+00 - mean_squared_error: 9.5607e-04 - val_loss: 0.6057 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0011\n",
      "Epoch 79/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6023 - acc: 0.0000e+00 - mean_squared_error: 9.0976e-04Epoch 00078: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6023 - acc: 0.0000e+00 - mean_squared_error: 9.0905e-04 - val_loss: 0.6051 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.4825e-04\n",
      "Epoch 80/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 8.1402e-04Epoch 00079: loss improved from 0.60224 to 0.60212, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 8.1404e-04 - val_loss: 0.6051 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.3890e-04\n",
      "Epoch 81/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6022 - acc: 0.0000e+00 - mean_squared_error: 8.6345e-04Epoch 00080: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6022 - acc: 0.0000e+00 - mean_squared_error: 8.6361e-04 - val_loss: 0.6053 - val_acc: 0.0000e+00 - val_mean_squared_error: 9.1888e-04\n",
      "Epoch 82/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6023 - acc: 0.0000e+00 - mean_squared_error: 8.2184e-04Epoch 00081: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 8.2300e-04 - val_loss: 0.6057 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0011\n",
      "Epoch 83/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6024 - acc: 0.0000e+00 - mean_squared_error: 9.4347e-04Epoch 00082: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6024 - acc: 0.0000e+00 - mean_squared_error: 9.4210e-04 - val_loss: 0.6053 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.9965e-04\n",
      "Epoch 84/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 8.3790e-04Epoch 00083: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6022 - acc: 0.0000e+00 - mean_squared_error: 8.3859e-04 - val_loss: 0.6057 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0011\n",
      "Epoch 85/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6024 - acc: 0.0000e+00 - mean_squared_error: 8.4657e-04Epoch 00084: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6022 - acc: 0.0000e+00 - mean_squared_error: 8.4633e-04 - val_loss: 0.6051 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.3828e-04\n",
      "Epoch 86/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6024 - acc: 0.0000e+00 - mean_squared_error: 8.9773e-04Epoch 00085: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6023 - acc: 0.0000e+00 - mean_squared_error: 8.9988e-04 - val_loss: 0.6058 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0011\n",
      "Epoch 87/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6027 - acc: 0.0000e+00 - mean_squared_error: 0.0010    Epoch 00086: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6026 - acc: 0.0000e+00 - mean_squared_error: 0.0010 - val_loss: 0.6051 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.5858e-04\n",
      "Epoch 88/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6023 - acc: 0.0000e+00 - mean_squared_error: 8.8036e-04Epoch 00087: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6023 - acc: 0.0000e+00 - mean_squared_error: 8.7892e-04 - val_loss: 0.6051 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.3497e-04\n",
      "Epoch 89/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6022 - acc: 0.0000e+00 - mean_squared_error: 8.1786e-04Epoch 00088: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 8.1749e-04 - val_loss: 0.6051 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.5429e-04\n",
      "Epoch 90/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6022 - acc: 0.0000e+00 - mean_squared_error: 8.0721e-04Epoch 00089: loss improved from 0.60212 to 0.60211, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 8.1012e-04 - val_loss: 0.6051 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.3997e-04\n",
      "Epoch 91/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6023 - acc: 0.0000e+00 - mean_squared_error: 8.6043e-04Epoch 00090: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6022 - acc: 0.0000e+00 - mean_squared_error: 8.6174e-04 - val_loss: 0.6053 - val_acc: 0.0000e+00 - val_mean_squared_error: 9.1487e-04\n",
      "Epoch 92/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 8.3791e-04Epoch 00091: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6022 - acc: 0.0000e+00 - mean_squared_error: 8.3939e-04 - val_loss: 0.6051 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.5822e-04\n",
      "Epoch 93/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6022 - acc: 0.0000e+00 - mean_squared_error: 8.6281e-04Epoch 00092: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6022 - acc: 0.0000e+00 - mean_squared_error: 8.6159e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.0455e-04\n",
      "Epoch 94/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6024 - acc: 0.0000e+00 - mean_squared_error: 9.0488e-04Epoch 00093: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6023 - acc: 0.0000e+00 - mean_squared_error: 9.0492e-04 - val_loss: 0.6057 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0011\n",
      "Epoch 95/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6022 - acc: 0.0000e+00 - mean_squared_error: 8.4269e-04Epoch 00094: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6022 - acc: 0.0000e+00 - mean_squared_error: 8.4163e-04 - val_loss: 0.6051 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.1971e-04\n",
      "Epoch 96/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 8.0822e-04Epoch 00095: loss improved from 0.60211 to 0.60210, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 8.0722e-04 - val_loss: 0.6051 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.4415e-04\n",
      "Epoch 97/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.9939e-04Epoch 00096: loss improved from 0.60210 to 0.60209, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 8.0006e-04 - val_loss: 0.6054 - val_acc: 0.0000e+00 - val_mean_squared_error: 9.4411e-04\n",
      "Epoch 98/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 8.0720e-04Epoch 00097: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 8.0837e-04 - val_loss: 0.6062 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0013\n",
      "Epoch 99/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6023 - acc: 0.0000e+00 - mean_squared_error: 8.7427e-04Epoch 00098: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6023 - acc: 0.0000e+00 - mean_squared_error: 8.7268e-04 - val_loss: 0.6052 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.9643e-04\n",
      "Epoch 100/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6024 - acc: 0.0000e+00 - mean_squared_error: 8.9510e-04Epoch 00099: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6023 - acc: 0.0000e+00 - mean_squared_error: 8.9609e-04 - val_loss: 0.6054 - val_acc: 0.0000e+00 - val_mean_squared_error: 9.6779e-04\n",
      "Epoch 101/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6022 - acc: 0.0000e+00 - mean_squared_error: 8.4006e-04Epoch 00100: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6022 - acc: 0.0000e+00 - mean_squared_error: 8.3795e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.1764e-04\n",
      "Epoch 102/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.8555e-04Epoch 00101: loss improved from 0.60209 to 0.60206, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 7.8502e-04 - val_loss: 0.6060 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012\n",
      "Epoch 103/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6024 - acc: 0.0000e+00 - mean_squared_error: 9.7339e-04Epoch 00102: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6025 - acc: 0.0000e+00 - mean_squared_error: 9.7206e-04 - val_loss: 0.6053 - val_acc: 0.0000e+00 - val_mean_squared_error: 9.1916e-04\n",
      "Epoch 104/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 8.0756e-04Epoch 00103: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 8.0773e-04 - val_loss: 0.6053 - val_acc: 0.0000e+00 - val_mean_squared_error: 9.2684e-04\n",
      "Epoch 105/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.7081e-04Epoch 00104: loss improved from 0.60206 to 0.60202, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.7074e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.8868e-04\n",
      "Epoch 106/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 7.6861e-04Epoch 00105: loss improved from 0.60202 to 0.60202, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.7010e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.8854e-04\n",
      "Epoch 107/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.8572e-04Epoch 00106: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 7.8511e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.1653e-04\n",
      "Epoch 108/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.5694e-04Epoch 00107: loss improved from 0.60202 to 0.60199, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.5579e-04 - val_loss: 0.6051 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.2034e-04\n",
      "Epoch 109/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 8.4853e-04Epoch 00108: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6022 - acc: 0.0000e+00 - mean_squared_error: 8.4985e-04 - val_loss: 0.6051 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.2453e-04\n",
      "Epoch 110/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.8609e-04Epoch 00109: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 7.8756e-04 - val_loss: 0.6054 - val_acc: 0.0000e+00 - val_mean_squared_error: 9.6467e-04\n",
      "Epoch 111/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6024 - acc: 0.0000e+00 - mean_squared_error: 8.7577e-04Epoch 00110: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6023 - acc: 0.0000e+00 - mean_squared_error: 8.7461e-04 - val_loss: 0.6051 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.2725e-04\n",
      "Epoch 112/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 8.2715e-04Epoch 00111: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6022 - acc: 0.0000e+00 - mean_squared_error: 8.2663e-04 - val_loss: 0.6053 - val_acc: 0.0000e+00 - val_mean_squared_error: 9.4702e-04\n",
      "Epoch 113/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6022 - acc: 0.0000e+00 - mean_squared_error: 7.9057e-04Epoch 00112: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 7.9698e-04 - val_loss: 0.6051 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.4674e-04\n",
      "Epoch 114/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6024 - acc: 0.0000e+00 - mean_squared_error: 9.4974e-04Epoch 00113: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6024 - acc: 0.0000e+00 - mean_squared_error: 9.5032e-04 - val_loss: 0.6053 - val_acc: 0.0000e+00 - val_mean_squared_error: 9.0830e-04\n",
      "Epoch 115/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6024 - acc: 0.0000e+00 - mean_squared_error: 9.0762e-04Epoch 00114: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6023 - acc: 0.0000e+00 - mean_squared_error: 9.0606e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.9243e-04\n",
      "Epoch 116/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.8160e-04Epoch 00115: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.7982e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.8559e-04\n",
      "Epoch 117/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6023 - acc: 0.0000e+00 - mean_squared_error: 7.6414e-04Epoch 00116: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.6320e-04 - val_loss: 0.6053 - val_acc: 0.0000e+00 - val_mean_squared_error: 9.1533e-04\n",
      "Epoch 118/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.9535e-04Epoch 00117: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 7.9591e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.0757e-04\n",
      "Epoch 119/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.7180e-04Epoch 00118: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.7168e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.6751e-04\n",
      "Epoch 120/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.8798e-04Epoch 00119: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 7.8876e-04 - val_loss: 0.6051 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.4182e-04\n",
      "Epoch 121/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 8.2191e-04Epoch 00120: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 8.2093e-04 - val_loss: 0.6052 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.8950e-04\n",
      "Epoch 122/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 8.4951e-04Epoch 00121: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6022 - acc: 0.0000e+00 - mean_squared_error: 8.4888e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.1551e-04\n",
      "Epoch 123/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6022 - acc: 0.0000e+00 - mean_squared_error: 7.7661e-04Epoch 00122: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.7534e-04 - val_loss: 0.6052 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.6870e-04\n",
      "Epoch 124/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 8.0375e-04Epoch 00123: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 8.0261e-04 - val_loss: 0.6057 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0011\n",
      "Epoch 125/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.8111e-04Epoch 00124: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.8079e-04 - val_loss: 0.6052 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.8304e-04\n",
      "Epoch 126/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.4653e-04Epoch 00125: loss improved from 0.60199 to 0.60196, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.4532e-04 - val_loss: 0.6054 - val_acc: 0.0000e+00 - val_mean_squared_error: 9.5259e-04\n",
      "Epoch 127/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 8.1899e-04Epoch 00126: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 8.1733e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.1460e-04\n",
      "Epoch 128/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.6771e-04Epoch 00127: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.6617e-04 - val_loss: 0.6051 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.1206e-04\n",
      "Epoch 129/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6022 - acc: 0.0000e+00 - mean_squared_error: 7.9714e-04Epoch 00128: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 7.9545e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.7169e-04\n",
      "Epoch 130/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.4833e-04Epoch 00129: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.4824e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.9638e-04\n",
      "Epoch 131/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6024 - acc: 0.0000e+00 - mean_squared_error: 7.5862e-04Epoch 00130: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.6181e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.4824e-04\n",
      "Epoch 132/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6023 - acc: 0.0000e+00 - mean_squared_error: 7.9519e-04Epoch 00131: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 7.9616e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.6347e-04\n",
      "Epoch 133/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.4692e-04Epoch 00132: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.4578e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.6367e-04\n",
      "Epoch 134/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 7.3180e-04Epoch 00133: loss improved from 0.60196 to 0.60193, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.3132e-04 - val_loss: 0.6051 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.5974e-04\n",
      "Epoch 135/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.4286e-04Epoch 00134: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.4164e-04 - val_loss: 0.6054 - val_acc: 0.0000e+00 - val_mean_squared_error: 9.4895e-04\n",
      "Epoch 136/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 7.4737e-04Epoch 00135: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.4660e-04 - val_loss: 0.6053 - val_acc: 0.0000e+00 - val_mean_squared_error: 9.4059e-04\n",
      "Epoch 137/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.4641e-04Epoch 00136: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.4697e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.4077e-04\n",
      "Epoch 138/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.6717e-04Epoch 00137: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.6851e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.3415e-04\n",
      "Epoch 139/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6022 - acc: 0.0000e+00 - mean_squared_error: 8.8466e-04Epoch 00138: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6023 - acc: 0.0000e+00 - mean_squared_error: 8.8345e-04 - val_loss: 0.6052 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.8753e-04\n",
      "Epoch 140/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6025 - acc: 0.0000e+00 - mean_squared_error: 8.6616e-04Epoch 00139: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6023 - acc: 0.0000e+00 - mean_squared_error: 8.6687e-04 - val_loss: 0.6058 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0011\n",
      "Epoch 141/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6023 - acc: 0.0000e+00 - mean_squared_error: 8.3914e-04Epoch 00140: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6022 - acc: 0.0000e+00 - mean_squared_error: 8.3756e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.6204e-04\n",
      "Epoch 142/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.1561e-04Epoch 00141: loss improved from 0.60193 to 0.60189, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.1646e-04 - val_loss: 0.6052 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.9547e-04\n",
      "Epoch 143/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.6376e-04Epoch 00142: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.6843e-04 - val_loss: 0.6051 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.4060e-04\n",
      "Epoch 144/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6022 - acc: 0.0000e+00 - mean_squared_error: 8.5694e-04Epoch 00143: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6022 - acc: 0.0000e+00 - mean_squared_error: 8.5447e-04 - val_loss: 0.6052 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.8779e-04\n",
      "Epoch 145/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.9442e-04Epoch 00144: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 7.9424e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.9570e-04\n",
      "Epoch 146/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.3353e-04Epoch 00145: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.3382e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.5205e-04\n",
      "Epoch 147/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.1020e-04Epoch 00146: loss improved from 0.60189 to 0.60188, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.1062e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.3367e-04\n",
      "Epoch 148/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.4999e-04Epoch 00147: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.5306e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.5422e-04\n",
      "Epoch 149/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.9616e-04Epoch 00148: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 7.9458e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.5288e-04\n",
      "Epoch 150/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 8.0826e-04Epoch 00149: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 8.0830e-04 - val_loss: 0.6053 - val_acc: 0.0000e+00 - val_mean_squared_error: 9.2214e-04\n",
      "Epoch 151/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 7.2974e-04Epoch 00150: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.3108e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.3494e-04\n",
      "Epoch 152/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.6292e-04Epoch 00151: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.6249e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.3858e-04\n",
      "Epoch 153/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.6427e-04Epoch 00152: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.6392e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.0482e-04\n",
      "Epoch 154/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 7.7388e-04Epoch 00153: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.7359e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.6402e-04\n",
      "Epoch 155/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "660/666 [============================>.] - ETA: 0s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 7.3362e-04Epoch 00154: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.3195e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.7728e-04\n",
      "Epoch 156/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.6239e-04Epoch 00155: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.6145e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.3207e-04\n",
      "Epoch 157/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.9848e-04Epoch 00156: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 7.9718e-04 - val_loss: 0.6054 - val_acc: 0.0000e+00 - val_mean_squared_error: 9.8277e-04\n",
      "Epoch 158/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.5981e-04Epoch 00157: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.5837e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.2143e-04\n",
      "Epoch 159/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 7.1470e-04Epoch 00158: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.1642e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.4352e-04\n",
      "Epoch 160/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6022 - acc: 0.0000e+00 - mean_squared_error: 8.1499e-04Epoch 00159: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 8.1589e-04 - val_loss: 0.6054 - val_acc: 0.0000e+00 - val_mean_squared_error: 9.5908e-04\n",
      "Epoch 161/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.4382e-04Epoch 00160: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.4392e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.6365e-04\n",
      "Epoch 162/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.3670e-04Epoch 00161: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.3872e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.2252e-04\n",
      "Epoch 163/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.6988e-04Epoch 00162: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.7027e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.2436e-04\n",
      "Epoch 164/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.4575e-04Epoch 00163: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.4487e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.6350e-04\n",
      "Epoch 165/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 7.3201e-04Epoch 00164: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.3122e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.2374e-04\n",
      "Epoch 166/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.4386e-04Epoch 00165: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.4500e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.8809e-04\n",
      "Epoch 167/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.3144e-04Epoch 00166: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.3539e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.0868e-04\n",
      "Epoch 168/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.3077e-04Epoch 00167: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.3174e-04 - val_loss: 0.6051 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.1943e-04\n",
      "Epoch 169/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.2000e-04Epoch 00168: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.2259e-04 - val_loss: 0.6051 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.4718e-04\n",
      "Epoch 170/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 7.2713e-04Epoch 00169: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.2629e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.1530e-04\n",
      "Epoch 171/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.2580e-04Epoch 00170: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.2699e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.4685e-04\n",
      "Epoch 172/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 6.9781e-04Epoch 00171: loss improved from 0.60188 to 0.60185, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.9722e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.8007e-04\n",
      "Epoch 173/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 7.1483e-04Epoch 00172: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.1409e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.3951e-04\n",
      "Epoch 174/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.3794e-04Epoch 00173: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.3914e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.4398e-04\n",
      "Epoch 175/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.2580e-04Epoch 00174: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.2787e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.1295e-04\n",
      "Epoch 176/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 7.4392e-04Epoch 00175: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.4464e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.2568e-04\n",
      "Epoch 177/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.7580e-04Epoch 00176: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.7513e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.1030e-04\n",
      "Epoch 178/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.4273e-04Epoch 00177: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.4195e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.1551e-04\n",
      "Epoch 179/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.1067e-04Epoch 00178: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.1004e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.2438e-04\n",
      "Epoch 180/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.9228e-04Epoch 00179: loss improved from 0.60185 to 0.60184, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.9205e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.2647e-04\n",
      "Epoch 181/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.0509e-04Epoch 00180: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.0565e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.5908e-04\n",
      "Epoch 182/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.1651e-04Epoch 00181: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.1634e-04 - val_loss: 0.6052 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.9510e-04\n",
      "Epoch 183/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.6092e-04Epoch 00182: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.6173e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.1410e-04\n",
      "Epoch 184/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 7.1986e-04Epoch 00183: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.1908e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.0154e-04\n",
      "Epoch 185/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 7.0522e-04Epoch 00184: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.0472e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.9976e-04\n",
      "Epoch 186/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 7.0473e-04Epoch 00185: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.1041e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.7721e-04\n",
      "Epoch 187/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.3337e-04Epoch 00186: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.3213e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.0538e-04\n",
      "Epoch 188/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.5362e-04Epoch 00187: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.5313e-04 - val_loss: 0.6053 - val_acc: 0.0000e+00 - val_mean_squared_error: 9.1835e-04\n",
      "Epoch 189/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.5238e-04Epoch 00188: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.5305e-04 - val_loss: 0.6052 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.9516e-04\n",
      "Epoch 190/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6025 - acc: 0.0000e+00 - mean_squared_error: 9.2737e-04Epoch 00189: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6024 - acc: 0.0000e+00 - mean_squared_error: 9.2567e-04 - val_loss: 0.6051 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.5002e-04\n",
      "Epoch 191/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 8.1150e-04Epoch 00190: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 8.1236e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.5992e-04\n",
      "Epoch 192/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 7.1433e-04Epoch 00191: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.1398e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.0551e-04\n",
      "Epoch 193/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 6.9540e-04Epoch 00192: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.9517e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.0421e-04\n",
      "Epoch 194/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.9011e-04Epoch 00193: loss improved from 0.60184 to 0.60183, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.8893e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.2820e-04\n",
      "Epoch 195/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 6.7512e-04Epoch 00194: loss improved from 0.60183 to 0.60179, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.7559e-04 - val_loss: 0.6051 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.2072e-04\n",
      "Epoch 196/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 6.8175e-04Epoch 00195: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.8145e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.1097e-04\n",
      "Epoch 197/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 7.2406e-04Epoch 00196: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.2421e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.3310e-04\n",
      "Epoch 198/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.7759e-04Epoch 00197: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.7689e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.2708e-04\n",
      "Epoch 199/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.8847e-04Epoch 00198: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.8800e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.0057e-04\n",
      "Epoch 200/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 7.0332e-04Epoch 00199: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.0392e-04 - val_loss: 0.6051 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.5555e-04\n",
      "Epoch 201/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.1764e-04Epoch 00200: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.1617e-04 - val_loss: 0.6055 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0010\n",
      "Epoch 202/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6022 - acc: 0.0000e+00 - mean_squared_error: 7.7881e-04Epoch 00201: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.7690e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.2837e-04\n",
      "Epoch 203/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 6.9827e-04Epoch 00202: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.9820e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.0725e-04\n",
      "Epoch 204/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 6.9550e-04Epoch 00203: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.9644e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.9939e-04\n",
      "Epoch 205/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 7.6717e-04Epoch 00204: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.6543e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.2517e-04\n",
      "Epoch 206/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.9591e-04Epoch 00205: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.9786e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.0367e-04\n",
      "Epoch 207/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.0591e-04Epoch 00206: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.0510e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.9700e-04\n",
      "Epoch 208/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.0779e-04Epoch 00207: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.0767e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.9858e-04\n",
      "Epoch 209/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 6.7795e-04Epoch 00208: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.7859e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.2807e-04\n",
      "Epoch 210/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.2333e-04Epoch 00209: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.2166e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.8355e-04\n",
      "Epoch 211/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.9058e-04Epoch 00210: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.9056e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.8648e-04\n",
      "Epoch 212/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 6.6611e-04Epoch 00211: loss improved from 0.60179 to 0.60177, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.6541e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.9431e-04\n",
      "Epoch 213/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.8752e-04Epoch 00212: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.8689e-04 - val_loss: 0.6052 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.8132e-04\n",
      "Epoch 214/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.5742e-04Epoch 00213: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.5564e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.8182e-04\n",
      "Epoch 215/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 7.1558e-04Epoch 00214: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.1472e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.1896e-04\n",
      "Epoch 216/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 6.8344e-04Epoch 00215: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.8249e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.0995e-04\n",
      "Epoch 217/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 6.7013e-04Epoch 00216: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.7712e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.0401e-04\n",
      "Epoch 218/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.4501e-04Epoch 00217: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.4549e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.0612e-04\n",
      "Epoch 219/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 7.0390e-04Epoch 00218: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.0305e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.1108e-04\n",
      "Epoch 220/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.8715e-04Epoch 00219: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.8681e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.1441e-04\n",
      "Epoch 221/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.9236e-04Epoch 00220: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.9355e-04 - val_loss: 0.6051 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.2540e-04\n",
      "Epoch 222/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 7.9881e-04Epoch 00221: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6021 - acc: 0.0000e+00 - mean_squared_error: 7.9912e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.3495e-04\n",
      "Epoch 223/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.5371e-04Epoch 00222: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.5171e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.8754e-04\n",
      "Epoch 224/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.6328e-04Epoch 00223: loss improved from 0.60177 to 0.60177, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.6331e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.6849e-04\n",
      "Epoch 225/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 7.1210e-04Epoch 00224: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.1372e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.3711e-04\n",
      "Epoch 226/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.9003e-04Epoch 00225: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.8980e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.3106e-04\n",
      "Epoch 227/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.8818e-04Epoch 00226: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.8845e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.2346e-04\n",
      "Epoch 228/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.8208e-04Epoch 00227: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.8138e-04 - val_loss: 0.6056 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0010\n",
      "Epoch 229/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6023 - acc: 0.0000e+00 - mean_squared_error: 8.5216e-04Epoch 00228: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6022 - acc: 0.0000e+00 - mean_squared_error: 8.5293e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.1384e-04\n",
      "Epoch 230/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 6.8473e-04Epoch 00229: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.8570e-04 - val_loss: 0.6054 - val_acc: 0.0000e+00 - val_mean_squared_error: 9.6727e-04\n",
      "Epoch 231/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.9565e-04Epoch 00230: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.9662e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.3247e-04\n",
      "Epoch 232/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.8488e-04Epoch 00231: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.8724e-04 - val_loss: 0.6056 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0010\n",
      "Epoch 233/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.3150e-04Epoch 00232: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.3039e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.8500e-04\n",
      "Epoch 234/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.9340e-04Epoch 00233: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.9333e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.8294e-04\n",
      "Epoch 235/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.7600e-04Epoch 00234: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.7513e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.7421e-04\n",
      "Epoch 236/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.5316e-04Epoch 00235: loss improved from 0.60177 to 0.60175, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.5433e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.6427e-04\n",
      "Epoch 237/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 6.6355e-04Epoch 00236: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.6372e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.7533e-04\n",
      "Epoch 238/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 6.5437e-04Epoch 00237: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.5917e-04 - val_loss: 0.6069 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0016\n",
      "Epoch 239/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6022 - acc: 0.0000e+00 - mean_squared_error: 8.6907e-04Epoch 00238: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6022 - acc: 0.0000e+00 - mean_squared_error: 8.6662e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.0031e-04\n",
      "Epoch 240/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.7848e-04Epoch 00239: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.7876e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.8399e-04\n",
      "Epoch 241/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.6284e-04Epoch 00240: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.6140e-04 - val_loss: 0.6051 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.3534e-04\n",
      "Epoch 242/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 7.1598e-04Epoch 00241: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.1583e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.9676e-04\n",
      "Epoch 243/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.3389e-04Epoch 00242: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.3478e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.1270e-04\n",
      "Epoch 244/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.7327e-04Epoch 00243: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.7307e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.9316e-04\n",
      "Epoch 245/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.6275e-04Epoch 00244: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.6258e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.7292e-04\n",
      "Epoch 246/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.2663e-04Epoch 00245: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.2532e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.2365e-04\n",
      "Epoch 247/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.6000e-04Epoch 00246: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.6053e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.3223e-04\n",
      "Epoch 248/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.6812e-04Epoch 00247: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.6955e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.9858e-04\n",
      "Epoch 249/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.8022e-04Epoch 00248: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.8018e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.3095e-04\n",
      "Epoch 250/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.1932e-04Epoch 00249: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.2073e-04 - val_loss: 0.6058 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0011\n",
      "Epoch 251/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.5765e-04Epoch 00250: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.5604e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.4276e-04\n",
      "Epoch 252/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.9278e-04Epoch 00251: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.9124e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.5036e-04\n",
      "Epoch 253/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 6.8403e-04Epoch 00252: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.8458e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.1050e-04\n",
      "Epoch 254/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.8621e-04Epoch 00253: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.8528e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.1313e-04\n",
      "Epoch 255/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.8957e-04Epoch 00254: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.8853e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.4893e-04\n",
      "Epoch 256/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.2029e-04Epoch 00255: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.2169e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.9918e-04\n",
      "Epoch 257/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.6765e-04Epoch 00256: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.6645e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.7558e-04\n",
      "Epoch 258/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.5451e-04Epoch 00257: loss improved from 0.60175 to 0.60174, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.5354e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.4619e-04\n",
      "Epoch 259/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 6.8400e-04Epoch 00258: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.8565e-04 - val_loss: 0.6051 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.3035e-04\n",
      "Epoch 260/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6022 - acc: 0.0000e+00 - mean_squared_error: 7.4772e-04Epoch 00259: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.4652e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.9467e-04\n",
      "Epoch 261/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.3490e-04Epoch 00260: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.3426e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.4444e-04\n",
      "Epoch 262/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.1049e-04Epoch 00261: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.1201e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.7467e-04\n",
      "Epoch 263/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.5397e-04Epoch 00262: loss improved from 0.60174 to 0.60174, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.5371e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.7270e-04\n",
      "Epoch 264/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.5021e-04Epoch 00263: loss improved from 0.60174 to 0.60174, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.5079e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.5187e-04\n",
      "Epoch 265/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.8354e-04Epoch 00264: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.8356e-04 - val_loss: 0.6051 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.3370e-04\n",
      "Epoch 266/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.7234e-04Epoch 00265: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.7097e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.7584e-04\n",
      "Epoch 267/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.4306e-04Epoch 00266: loss improved from 0.60174 to 0.60172, saving model to ./models/simple_cnn_oct.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.4246e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.5019e-04\n",
      "Epoch 268/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.1764e-04Epoch 00267: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.1704e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.2323e-04\n",
      "Epoch 269/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 7.0058e-04Epoch 00268: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 6.9937e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.8925e-04\n",
      "Epoch 270/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.4549e-04Epoch 00269: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.4595e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.7484e-04\n",
      "Epoch 271/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 6.8652e-04Epoch 00270: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.8549e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.8500e-04\n",
      "Epoch 272/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.5388e-04Epoch 00271: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.5327e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.6784e-04\n",
      "Epoch 273/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.6968e-04Epoch 00272: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.6988e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.9910e-04\n",
      "Epoch 274/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.0338e-04Epoch 00273: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.0173e-04 - val_loss: 0.6051 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.3135e-04\n",
      "Epoch 275/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.4750e-04Epoch 00274: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.4629e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.0884e-04\n",
      "Epoch 276/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 6.9462e-04Epoch 00275: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.9414e-04 - val_loss: 0.6054 - val_acc: 0.0000e+00 - val_mean_squared_error: 9.8485e-04\n",
      "Epoch 277/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.6345e-04Epoch 00276: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.6211e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.5884e-04\n",
      "Epoch 278/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 6.4926e-04Epoch 00277: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.4779e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.9753e-04\n",
      "Epoch 279/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.3988e-04Epoch 00278: loss improved from 0.60172 to 0.60171, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.4006e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.7003e-04\n",
      "Epoch 280/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 6.8848e-04Epoch 00279: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.8836e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.6125e-04\n",
      "Epoch 281/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 6.5943e-04Epoch 00280: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.5863e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.6744e-04\n",
      "Epoch 282/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.4677e-04Epoch 00281: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.4641e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.4780e-04\n",
      "Epoch 283/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.3722e-04Epoch 00282: loss improved from 0.60171 to 0.60171, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.3776e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.7050e-04\n",
      "Epoch 284/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 6.4401e-04Epoch 00283: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.4503e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.3774e-04\n",
      "Epoch 285/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 6.8174e-04Epoch 00284: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.8181e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.4894e-04\n",
      "Epoch 286/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.3478e-04Epoch 00285: loss improved from 0.60171 to 0.60170, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.3463e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.5508e-04\n",
      "Epoch 287/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.3286e-04Epoch 00286: loss improved from 0.60170 to 0.60170, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.3561e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.9737e-04\n",
      "Epoch 288/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.7938e-04Epoch 00287: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.7905e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.1363e-04\n",
      "Epoch 289/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.7158e-04Epoch 00288: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.7028e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.7796e-04\n",
      "Epoch 290/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.4144e-04Epoch 00289: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.4176e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.4782e-04\n",
      "Epoch 291/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.3696e-04Epoch 00290: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.3729e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.6043e-04\n",
      "Epoch 292/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.4478e-04Epoch 00291: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.4414e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.7817e-04\n",
      "Epoch 293/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 6.7852e-04Epoch 00292: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.7834e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.5158e-04\n",
      "Epoch 294/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.5199e-04Epoch 00293: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.5139e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.4853e-04\n",
      "Epoch 295/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.5414e-04Epoch 00294: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.5349e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.2038e-04\n",
      "Epoch 296/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 6.8227e-04Epoch 00295: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.8224e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.8848e-04\n",
      "Epoch 297/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 6.7309e-04Epoch 00296: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.7835e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.6694e-04\n",
      "Epoch 298/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.0974e-04Epoch 00297: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.0939e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.0524e-04\n",
      "Epoch 299/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.7832e-04Epoch 00298: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.7955e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.4431e-04\n",
      "Epoch 300/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.7777e-04Epoch 00299: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.7904e-04 - val_loss: 0.6052 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.5764e-04\n",
      "Epoch 301/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6022 - acc: 0.0000e+00 - mean_squared_error: 7.6135e-04Epoch 00300: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.5975e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.5721e-04\n",
      "Epoch 302/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.5832e-04Epoch 00301: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.5814e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.4637e-04\n",
      "Epoch 303/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.4356e-04Epoch 00302: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.4400e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.3644e-04\n",
      "Epoch 304/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 6.6939e-04Epoch 00303: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.6765e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.4452e-04\n",
      "Epoch 305/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.6992e-04Epoch 00304: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.6912e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.4704e-04\n",
      "Epoch 306/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.3192e-04Epoch 00305: loss improved from 0.60170 to 0.60169, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.3199e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.8534e-04\n",
      "Epoch 307/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.3647e-04Epoch 00306: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.3584e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.7154e-04\n",
      "Epoch 308/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.5052e-04Epoch 00307: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.4970e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.4032e-04\n",
      "Epoch 309/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.5774e-04Epoch 00308: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.5773e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.2902e-04\n",
      "Epoch 310/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.4638e-04Epoch 00309: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.4718e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.5802e-04\n",
      "Epoch 311/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.4305e-04Epoch 00310: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.4366e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.4779e-04\n",
      "Epoch 312/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 7.0088e-04Epoch 00311: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.0170e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.0237e-04\n",
      "Epoch 313/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.4626e-04Epoch 00312: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.4609e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.4052e-04\n",
      "Epoch 314/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.2026e-04Epoch 00313: loss improved from 0.60169 to 0.60166, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.2024e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.4618e-04\n",
      "Epoch 315/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.5945e-04Epoch 00314: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.5820e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.8727e-04\n",
      "Epoch 316/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.5752e-04Epoch 00315: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.5805e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.8009e-04\n",
      "Epoch 317/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.5046e-04Epoch 00316: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.4982e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.5298e-04\n",
      "Epoch 318/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.2683e-04Epoch 00317: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.2653e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.6635e-04\n",
      "Epoch 319/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6015 - acc: 0.0000e+00 - mean_squared_error: 6.1889e-04Epoch 00318: loss improved from 0.60166 to 0.60166, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.1845e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.4962e-04\n",
      "Epoch 320/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.1596e-04Epoch 00319: loss improved from 0.60166 to 0.60166, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.1619e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.2710e-04\n",
      "Epoch 321/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.4491e-04Epoch 00320: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.4482e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.5983e-04\n",
      "Epoch 322/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.5886e-04Epoch 00321: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.5851e-04 - val_loss: 0.6059 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012\n",
      "Epoch 323/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.4045e-04Epoch 00322: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.3873e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.4851e-04\n",
      "Epoch 324/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.2146e-04Epoch 00323: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.2179e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.6792e-04\n",
      "Epoch 325/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.4302e-04Epoch 00324: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.4199e-04 - val_loss: 0.6054 - val_acc: 0.0000e+00 - val_mean_squared_error: 9.8483e-04\n",
      "Epoch 326/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.7023e-04Epoch 00325: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.7022e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.6627e-04\n",
      "Epoch 327/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.4390e-04Epoch 00326: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.4378e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.9128e-04\n",
      "Epoch 328/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.2981e-04Epoch 00327: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.3147e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.2902e-04\n",
      "Epoch 329/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 6.6858e-04Epoch 00328: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.6832e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.4339e-04\n",
      "Epoch 330/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.1576e-04Epoch 00329: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.1616e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.0175e-04\n",
      "Epoch 331/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.6551e-04Epoch 00330: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.6680e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.6129e-04\n",
      "Epoch 332/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.5150e-04Epoch 00331: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.5530e-04 - val_loss: 0.6053 - val_acc: 0.0000e+00 - val_mean_squared_error: 9.2550e-04\n",
      "Epoch 333/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6023 - acc: 0.0000e+00 - mean_squared_error: 8.8514e-04Epoch 00332: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6023 - acc: 0.0000e+00 - mean_squared_error: 8.8259e-04 - val_loss: 0.6054 - val_acc: 0.0000e+00 - val_mean_squared_error: 9.7354e-04\n",
      "Epoch 334/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.1016e-04Epoch 00333: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.0881e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.3258e-04\n",
      "Epoch 335/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 6.5070e-04Epoch 00334: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.5120e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.8194e-04\n",
      "Epoch 336/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 7.0010e-04Epoch 00335: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 6.9963e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.4124e-04\n",
      "Epoch 337/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.2533e-04Epoch 00336: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.2534e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.3706e-04\n",
      "Epoch 338/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.3633e-04Epoch 00337: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.3543e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.6932e-04\n",
      "Epoch 339/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.1880e-04Epoch 00338: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.1859e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.5783e-04\n",
      "Epoch 340/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 6.5475e-04Epoch 00339: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.5504e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.6059e-04\n",
      "Epoch 341/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.2607e-04Epoch 00340: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.2497e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.3243e-04\n",
      "Epoch 342/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0376e-04Epoch 00341: loss improved from 0.60166 to 0.60163, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0450e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.6129e-04\n",
      "Epoch 343/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.2828e-04Epoch 00342: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.2861e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.0424e-04\n",
      "Epoch 344/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.3638e-04Epoch 00343: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.3504e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.1527e-04\n",
      "Epoch 345/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 6.8930e-04Epoch 00344: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.8951e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.8242e-04\n",
      "Epoch 346/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.6177e-04Epoch 00345: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.6447e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.3506e-04\n",
      "Epoch 347/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 7.0046e-04Epoch 00346: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 6.9919e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.6967e-04\n",
      "Epoch 348/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.4707e-04Epoch 00347: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.4593e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.7470e-04\n",
      "Epoch 349/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.3254e-04Epoch 00348: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.3276e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.4381e-04\n",
      "Epoch 350/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.2367e-04Epoch 00349: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.2353e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.2979e-04\n",
      "Epoch 351/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.7034e-04Epoch 00350: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.7023e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.6669e-04\n",
      "Epoch 352/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.4243e-04Epoch 00351: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.4211e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.8106e-04\n",
      "Epoch 353/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 6.7636e-04Epoch 00352: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.7490e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.3371e-04\n",
      "Epoch 354/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.5895e-04Epoch 00353: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.5924e-04 - val_loss: 0.6053 - val_acc: 0.0000e+00 - val_mean_squared_error: 9.3626e-04\n",
      "Epoch 355/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.9440e-04Epoch 00354: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.9351e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.5123e-04\n",
      "Epoch 356/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.3671e-04Epoch 00355: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.3780e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.5628e-04\n",
      "Epoch 357/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.1387e-04Epoch 00356: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.1434e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.3591e-04\n",
      "Epoch 358/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.3129e-04Epoch 00357: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.3036e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.3380e-04\n",
      "Epoch 359/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.2595e-04Epoch 00358: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.2499e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.5146e-04\n",
      "Epoch 360/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.1948e-04Epoch 00359: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.2068e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.4043e-04\n",
      "Epoch 361/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.1765e-04Epoch 00360: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.1591e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.2337e-04\n",
      "Epoch 362/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.3492e-04Epoch 00361: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.3572e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.0407e-04\n",
      "Epoch 363/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.1021e-04Epoch 00362: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.1101e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.3568e-04\n",
      "Epoch 364/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 6.1046e-04Epoch 00363: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.1030e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.1441e-04\n",
      "Epoch 365/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.1873e-04Epoch 00364: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.1783e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.1510e-04\n",
      "Epoch 366/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.1963e-04Epoch 00365: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.2057e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.4082e-04\n",
      "Epoch 367/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.2589e-04Epoch 00366: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.2594e-04 - val_loss: 0.6053 - val_acc: 0.0000e+00 - val_mean_squared_error: 9.0970e-04\n",
      "Epoch 368/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.3554e-04Epoch 00367: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.3590e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.9636e-04\n",
      "Epoch 369/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.3184e-04Epoch 00368: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.3139e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.7222e-04\n",
      "Epoch 370/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.4794e-04Epoch 00369: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.5175e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.6798e-04\n",
      "Epoch 371/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.6133e-04Epoch 00370: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.6130e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.3736e-04\n",
      "Epoch 372/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.1737e-04Epoch 00371: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.1708e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.6875e-04\n",
      "Epoch 373/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.2713e-04Epoch 00372: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.2600e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.2897e-04\n",
      "Epoch 374/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.2564e-04Epoch 00373: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.2643e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.2885e-04\n",
      "Epoch 375/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.5378e-04Epoch 00374: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.5326e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.2647e-04\n",
      "Epoch 376/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.1413e-04Epoch 00375: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.1477e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.2444e-04\n",
      "Epoch 377/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.1529e-04Epoch 00376: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.1452e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.8336e-04\n",
      "Epoch 378/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.8932e-04Epoch 00377: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.8977e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.4544e-04\n",
      "Epoch 379/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.3454e-04Epoch 00378: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.3465e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.5171e-04\n",
      "Epoch 380/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.5077e-04Epoch 00379: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.5002e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.9493e-04\n",
      "Epoch 381/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.1027e-04Epoch 00380: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0963e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.2672e-04\n",
      "Epoch 382/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.3021e-04Epoch 00381: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.2966e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.9966e-04\n",
      "Epoch 383/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.5491e-04Epoch 00382: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.5435e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.6554e-04\n",
      "Epoch 384/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.2471e-04Epoch 00383: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.2443e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.9032e-04\n",
      "Epoch 385/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0889e-04Epoch 00384: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0964e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.5337e-04\n",
      "Epoch 386/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.1495e-04Epoch 00385: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.1474e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.2714e-04\n",
      "Epoch 387/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.4363e-04Epoch 00386: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.4275e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.9970e-04\n",
      "Epoch 388/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.2908e-04Epoch 00387: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.2842e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.3720e-04\n",
      "Epoch 389/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0221e-04Epoch 00388: loss improved from 0.60163 to 0.60162, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0263e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.3266e-04\n",
      "Epoch 390/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.2768e-04Epoch 00389: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.2686e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.2985e-04\n",
      "Epoch 391/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.1007e-04Epoch 00390: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.1057e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.3360e-04\n",
      "Epoch 392/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0837e-04Epoch 00391: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0798e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.0941e-04\n",
      "Epoch 393/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.6286e-04Epoch 00392: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.6588e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.5340e-04\n",
      "Epoch 394/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 7.1703e-04Epoch 00393: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.1756e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.9086e-04\n",
      "Epoch 395/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.1713e-04Epoch 00394: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.1815e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.2257e-04\n",
      "Epoch 396/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.2194e-04Epoch 00395: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.2294e-04 - val_loss: 0.6052 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.6603e-04\n",
      "Epoch 397/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.0510e-04Epoch 00396: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.0501e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.6065e-04\n",
      "Epoch 398/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.4502e-04Epoch 00397: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.4527e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.0260e-04\n",
      "Epoch 399/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.7990e-04Epoch 00398: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.7914e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.8466e-04\n",
      "Epoch 400/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.5949e-04Epoch 00399: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.5881e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.7669e-04\n",
      "Epoch 401/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.1161e-04Epoch 00400: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.1121e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.2971e-04\n",
      "Epoch 402/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.3687e-04Epoch 00401: loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.3690e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.1137e-04\n",
      "Epoch 403/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.2102e-04Epoch 00402: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.2100e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.2862e-04\n",
      "Epoch 404/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.2611e-04Epoch 00403: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.2494e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.3283e-04\n",
      "Epoch 405/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.0198e-04Epoch 00404: loss improved from 0.60162 to 0.60162, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0190e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.1692e-04\n",
      "Epoch 406/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0089e-04Epoch 00405: loss improved from 0.60162 to 0.60162, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0027e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.1113e-04\n",
      "Epoch 407/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.3123e-04Epoch 00406: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.3130e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.2710e-04\n",
      "Epoch 408/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.0488e-04Epoch 00407: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0459e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.1940e-04\n",
      "Epoch 409/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0893e-04Epoch 00408: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0753e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.1486e-04\n",
      "Epoch 410/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.2570e-04Epoch 00409: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.2470e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.3321e-04\n",
      "Epoch 411/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0947e-04Epoch 00410: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0867e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.4407e-04\n",
      "Epoch 412/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.1941e-04Epoch 00411: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.1822e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.1907e-04\n",
      "Epoch 413/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.1065e-04Epoch 00412: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0963e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.1645e-04\n",
      "Epoch 414/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6015 - acc: 0.0000e+00 - mean_squared_error: 6.0810e-04Epoch 00413: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0937e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.1682e-04\n",
      "Epoch 415/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.3107e-04Epoch 00414: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.3406e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.3137e-04\n",
      "Epoch 416/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.9930e-04Epoch 00415: loss improved from 0.60162 to 0.60161, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.9886e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.8733e-04\n",
      "Epoch 417/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.1002e-04Epoch 00416: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0905e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.3310e-04\n",
      "Epoch 418/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.3615e-04Epoch 00417: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.3601e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.4468e-04\n",
      "Epoch 419/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.1560e-04Epoch 00418: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.1572e-04 - val_loss: 0.6052 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.9362e-04\n",
      "Epoch 420/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.3194e-04Epoch 00419: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.3125e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.1283e-04\n",
      "Epoch 421/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.1147e-04Epoch 00420: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.1303e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.5762e-04\n",
      "Epoch 422/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.4178e-04Epoch 00421: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.4241e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.6750e-04\n",
      "Epoch 423/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0711e-04Epoch 00422: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0674e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.0394e-04\n",
      "Epoch 424/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.9862e-04Epoch 00423: loss improved from 0.60161 to 0.60161, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.9804e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.2636e-04\n",
      "Epoch 425/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.1421e-04Epoch 00424: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.1378e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.3622e-04\n",
      "Epoch 426/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0961e-04Epoch 00425: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0927e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.6385e-04\n",
      "Epoch 427/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.1826e-04Epoch 00426: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.1844e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.7486e-04\n",
      "Epoch 428/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.3348e-04Epoch 00427: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.3352e-04 - val_loss: 0.6051 - val_acc: 0.0000e+00 - val_mean_squared_error: 8.4326e-04\n",
      "Epoch 429/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.2365e-04Epoch 00428: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.2312e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.7096e-04\n",
      "Epoch 430/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.1725e-04Epoch 00429: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.1747e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.6112e-04\n",
      "Epoch 431/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.1658e-04Epoch 00430: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.1646e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.6859e-04\n",
      "Epoch 432/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.3311e-04Epoch 00431: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.3258e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.7170e-04\n",
      "Epoch 433/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0914e-04Epoch 00432: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.1141e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.6737e-04\n",
      "Epoch 434/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.7922e-04Epoch 00433: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.7776e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.7350e-04\n",
      "Epoch 435/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 6.1217e-04Epoch 00434: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.1191e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.1873e-04\n",
      "Epoch 436/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.9713e-04Epoch 00435: loss improved from 0.60161 to 0.60161, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.9624e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.6548e-04\n",
      "Epoch 437/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.2889e-04Epoch 00436: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.2876e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.9474e-04\n",
      "Epoch 438/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.1741e-04Epoch 00437: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.1797e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.2130e-04\n",
      "Epoch 439/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.1013e-04Epoch 00438: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.1191e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.1787e-04\n",
      "Epoch 440/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.3483e-04Epoch 00439: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.3412e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.3225e-04\n",
      "Epoch 441/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.0063e-04Epoch 00440: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.9995e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.2809e-04\n",
      "Epoch 442/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6015 - acc: 0.0000e+00 - mean_squared_error: 6.0812e-04Epoch 00441: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0795e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.1962e-04\n",
      "Epoch 443/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6015 - acc: 0.0000e+00 - mean_squared_error: 5.9163e-04Epoch 00442: loss improved from 0.60161 to 0.60160, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.9133e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.2063e-04\n",
      "Epoch 444/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.2393e-04Epoch 00443: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.2251e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.5432e-04\n",
      "Epoch 445/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.7831e-04Epoch 00444: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.7831e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.5779e-04\n",
      "Epoch 446/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.1704e-04Epoch 00445: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.1661e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.3265e-04\n",
      "Epoch 447/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.9813e-04Epoch 00446: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.9780e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.8982e-04\n",
      "Epoch 448/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0628e-04Epoch 00447: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0615e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.5683e-04\n",
      "Epoch 449/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.7192e-04Epoch 00448: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.7082e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.4135e-04\n",
      "Epoch 450/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.1701e-04Epoch 00449: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.1743e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.3384e-04\n",
      "Epoch 451/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.2633e-04Epoch 00450: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.2551e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.6255e-04\n",
      "Epoch 452/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.6925e-04Epoch 00451: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.6959e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.2804e-04\n",
      "Epoch 453/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.3252e-04Epoch 00452: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.3231e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.2628e-04\n",
      "Epoch 454/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.9630e-04Epoch 00453: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.9728e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.1653e-04\n",
      "Epoch 455/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.8582e-04Epoch 00454: loss improved from 0.60160 to 0.60158, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.8493e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.5071e-04\n",
      "Epoch 456/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.4880e-04Epoch 00455: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.4794e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.3565e-04\n",
      "Epoch 457/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.2792e-04Epoch 00456: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.2705e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.2164e-04\n",
      "Epoch 458/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 5.9768e-04Epoch 00457: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.9648e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.7471e-04\n",
      "Epoch 459/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 5.9894e-04Epoch 00458: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.9811e-04 - val_loss: 0.6045 - val_acc: 0.0000e+00 - val_mean_squared_error: 5.9707e-04\n",
      "Epoch 460/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6015 - acc: 0.0000e+00 - mean_squared_error: 5.7782e-04Epoch 00459: loss improved from 0.60158 to 0.60156, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.7823e-04 - val_loss: 0.6045 - val_acc: 0.0000e+00 - val_mean_squared_error: 5.9942e-04\n",
      "Epoch 461/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 5.8088e-04Epoch 00460: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.8066e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.1411e-04\n",
      "Epoch 462/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 5.9639e-04Epoch 00461: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.9666e-04 - val_loss: 0.6045 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.0695e-04\n",
      "Epoch 463/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.9646e-04Epoch 00462: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.9643e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.1878e-04\n",
      "Epoch 464/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.1428e-04Epoch 00463: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.1458e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.2886e-04\n",
      "Epoch 465/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 6.1736e-04Epoch 00464: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.1691e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.7074e-04\n",
      "Epoch 466/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6015 - acc: 0.0000e+00 - mean_squared_error: 6.0347e-04Epoch 00465: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0329e-04 - val_loss: 0.6045 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.0232e-04\n",
      "Epoch 467/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.1774e-04Epoch 00466: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.1718e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.5190e-04\n",
      "Epoch 468/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.3006e-04Epoch 00467: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.2967e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.8531e-04\n",
      "Epoch 469/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.5272e-04Epoch 00468: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.5207e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.8448e-04\n",
      "Epoch 470/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 5.8803e-04Epoch 00469: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.8765e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.7465e-04\n",
      "Epoch 471/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0558e-04Epoch 00470: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0559e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.3203e-04\n",
      "Epoch 472/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.9628e-04Epoch 00471: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.9580e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.2648e-04\n",
      "Epoch 473/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.2288e-04Epoch 00472: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.2207e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.4165e-04\n",
      "Epoch 474/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.1571e-04Epoch 00473: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.1543e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.2619e-04\n",
      "Epoch 475/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6015 - acc: 0.0000e+00 - mean_squared_error: 5.9320e-04Epoch 00474: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.9259e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.7540e-04\n",
      "Epoch 476/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 5.7854e-04Epoch 00475: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.7805e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.1536e-04\n",
      "Epoch 477/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.1384e-04Epoch 00476: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.1386e-04 - val_loss: 0.6045 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.0472e-04\n",
      "Epoch 478/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.7506e-04Epoch 00477: loss improved from 0.60156 to 0.60156, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.7565e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.5466e-04\n",
      "Epoch 479/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.0000e+00 - mean_squared_error: 5.8573e-04Epoch 00478: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.8649e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.3313e-04\n",
      "Epoch 480/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 6.1456e-04Epoch 00479: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.1368e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.0087e-04\n",
      "Epoch 481/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 5.9053e-04Epoch 00480: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.9103e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.4673e-04\n",
      "Epoch 482/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.1897e-04Epoch 00481: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.1833e-04 - val_loss: 0.6050 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.6762e-04\n",
      "Epoch 483/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0776e-04Epoch 00482: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0745e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.0712e-04\n",
      "Epoch 484/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.9592e-04Epoch 00483: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.9520e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.7883e-04\n",
      "Epoch 485/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.9948e-04Epoch 00484: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.9868e-04 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_mean_squared_error: 7.4212e-04\n",
      "Epoch 486/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6015 - acc: 0.0000e+00 - mean_squared_error: 6.0004e-04Epoch 00485: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0038e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.1331e-04\n",
      "Epoch 487/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 6.0110e-04Epoch 00486: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0112e-04 - val_loss: 0.6059 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012\n",
      "Epoch 488/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.0000e+00 - mean_squared_error: 7.1914e-04Epoch 00487: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6019 - acc: 0.0000e+00 - mean_squared_error: 7.1868e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.2753e-04\n",
      "Epoch 489/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.9308e-04Epoch 00488: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.9267e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.1997e-04\n",
      "Epoch 490/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6015 - acc: 0.0000e+00 - mean_squared_error: 5.8257e-04Epoch 00489: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.8284e-04 - val_loss: 0.6045 - val_acc: 0.0000e+00 - val_mean_squared_error: 5.9775e-04\n",
      "Epoch 491/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6015 - acc: 0.0000e+00 - mean_squared_error: 5.8750e-04Epoch 00490: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.8753e-04 - val_loss: 0.6048 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.9256e-04\n",
      "Epoch 492/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.0374e-04Epoch 00491: loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0320e-04 - val_loss: 0.6045 - val_acc: 0.0000e+00 - val_mean_squared_error: 5.9663e-04\n",
      "Epoch 493/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6015 - acc: 0.0000e+00 - mean_squared_error: 5.7470e-04Epoch 00492: loss improved from 0.60156 to 0.60156, saving model to ./models/simple_cnn_oct.hdf5\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.7459e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.2028e-04\n",
      "Epoch 494/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.2713e-04Epoch 00493: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.2781e-04 - val_loss: 0.6047 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.7466e-04\n",
      "Epoch 495/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 5.9513e-04Epoch 00494: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.9491e-04 - val_loss: 0.6045 - val_acc: 0.0000e+00 - val_mean_squared_error: 5.9256e-04\n",
      "Epoch 496/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.8456e-04Epoch 00495: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.8406e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.0978e-04\n",
      "Epoch 497/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.8923e-04Epoch 00496: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 5.8910e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.1282e-04\n",
      "Epoch 498/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.2668e-04Epoch 00497: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6017 - acc: 0.0000e+00 - mean_squared_error: 6.2648e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.2648e-04\n",
      "Epoch 499/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0562e-04Epoch 00498: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0636e-04 - val_loss: 0.6045 - val_acc: 0.0000e+00 - val_mean_squared_error: 5.9804e-04\n",
      "Epoch 500/500\n",
      "660/666 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0533e-04Epoch 00499: loss did not improve\n",
      "666/666 [==============================] - 48s - loss: 0.6016 - acc: 0.0000e+00 - mean_squared_error: 6.0443e-04 - val_loss: 0.6046 - val_acc: 0.0000e+00 - val_mean_squared_error: 6.3950e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa573d27f50>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(x_train_noisy, x_train,\n",
    "                epochs=500,\n",
    "                batch_size=20,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test_noisy, x_test),\n",
    "                callbacks=[TensorBoard(log_dir='/tmp/autoencoder', histogram_freq=0, write_graph=True), model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras_contrib.losses import DSSIMObjective\n",
    "import keras_contrib.backend as KC\n",
    "\n",
    "#Function to get saved keras model\n",
    "def read_model_json(jsonfilePath,h5filePath):\n",
    "    try:\n",
    "        json_file = open(jsonfilePath, 'r')\n",
    "        print json_file\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        print \"hello\"\n",
    "        loaded_model = model_from_json(loaded_model_json)\n",
    "         \n",
    "        # load weights into new model\n",
    "        loaded_model.load_weights(h5filePath)\n",
    "\n",
    "        return loaded_model\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def ms_ssim_loss(y_true,y_pred):\n",
    "        print y_true.shape,y_pred.shape\n",
    "        kernel = [3,3]\n",
    "        y_true = KC.reshape(y_true, [-1] + list(self.__int_shape(y_pred)[1:]))\n",
    "        y_pred = KC.reshape(y_pred, [-1] + list(self.__int_shape(y_pred)[1:]))\n",
    "\n",
    "        patches_pred = KC.extract_image_patches(y_pred, kernel, kernel, 'valid', self.dim_ordering)\n",
    "        patches_true = KC.extract_image_patches(y_true, kernel, kernel, 'valid', self.dim_ordering)\n",
    "\n",
    "        # Reshape to get the var in the cells\n",
    "        bs, w, h, c1, c2, c3 = self.__int_shape(patches_pred)\n",
    "        patches_pred = KC.reshape(patches_pred, [-1, w, h, c1 * c2 * c3])\n",
    "        patches_true = KC.reshape(patches_true, [-1, w, h, c1 * c2 * c3])\n",
    "        # Get mean\n",
    "        u_true = KC.mean(patches_true, axis=-1)\n",
    "        u_pred = KC.mean(patches_pred, axis=-1)\n",
    "        # Get variance\n",
    "        var_true = K.var(patches_true, axis=-1)\n",
    "        var_pred = K.var(patches_pred, axis=-1)\n",
    "        # Get std dev\n",
    "        covar_true_pred = K.mean(patches_true * patches_pred, axis=-1) - u_true * u_pred\n",
    "\n",
    "        ssim = (2 * u_true * u_pred + self.c1) * (2 * covar_true_pred + self.c2)\n",
    "        denom = (K.square(u_true) + K.square(u_pred) + self.c1) * (var_pred + var_true + self.c2)\n",
    "        ssim /= denom  # no need for clipping, c1 and c2 make the denom non-zero\n",
    "        return K.mean((1.0 - ssim) / 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gated_connections(gatePercentageFactor,inputLayer):\n",
    "    #gateFactor = Input(tensor = K.variable([gatePercentageFactor]))\n",
    "    fractionG = Lambda(lambda x, factor : factor * x, arguments = {'factor': gatePercentageFactor})(inputLayer)\n",
    "    complement = Lambda(lambda x: x[0] - x[1])([inputLayer,fractionG])\n",
    "    \n",
    "    return fractionG,complement\n",
    "\n",
    "#x is conv layer\n",
    "#y is de-conv layer\n",
    "#gf is gating factor\n",
    "#fg is fractional input from gate\n",
    "#c is complement ie remaining fraction from the gate\n",
    "#jt joining tensor of convolution layer and previous de-conv layer \n",
    "\n",
    "def get_cnn_dsc_architecture(model_path=None):\n",
    "    \n",
    "    if(model_path is None):\n",
    "        sym_autoencoder = None\n",
    "    else:\n",
    "        sym_autoencoder = read_model_json(model_path[0],model_path[1])\n",
    "        print model_path[0],model_path[1]\n",
    "    if(sym_autoencoder is None):\n",
    "        input_img = Input(shape=(None,None,1), name = \"Image_input\")  # adapt this if using `channels_first` image data format\n",
    "        x1 = Conv2D(64, (4, 4), activation='relu', padding='same')(input_img)\n",
    "        fg1,c1 = get_gated_connections(0.1,x1)\n",
    "        \n",
    "        x = MaxPooling2D((2, 2), padding='same')(fg1)\n",
    "        x2 = Conv2D(64, (4, 4), activation='relu', padding='same')(x) \n",
    "        fg2,c2 = get_gated_connections(0.2,x2)\n",
    "\n",
    "        x = MaxPooling2D((2, 2), padding='same')(fg2)\n",
    "        x3 = Conv2D(128, (4, 4), activation='relu', padding='same')(x) \n",
    "        fg3,c3 = get_gated_connections(0.3,x3)\n",
    "\n",
    "        x = MaxPooling2D((2, 2), padding='same')(fg3)\n",
    "        x4 = Conv2D(256, (4, 4), activation='relu', padding='same')(x) \n",
    "        fg4,c4 = get_gated_connections(0.4,x4)\n",
    "\n",
    "        x = MaxPooling2D((2, 2), padding='same')(fg4)\n",
    "        x5 = Conv2D(512, (4, 4), activation='relu', padding='same')(x) \n",
    "\n",
    "        x = UpSampling2D((2, 2))(x5)\n",
    "        y1 = Conv2DTranspose(256, (4, 4), activation='relu', padding='same')(x) \n",
    "        jt4 = Add()([y1,c4])\n",
    "        x = UpSampling2D((2, 2))(jt4)\n",
    "\n",
    "        y2 = Conv2DTranspose(128, (4, 4), activation='relu', padding='same')(x) \n",
    "        jt3 = Add()([y2,c3])\n",
    "        x = UpSampling2D((2, 2))(jt3)\n",
    "\n",
    "        y3 = Conv2DTranspose(64, (4, 4), activation='relu', padding='same')(x) \n",
    "        jt2 = Add()([y3,c2])\n",
    "        x = UpSampling2D((2, 2))(jt2)\n",
    "\n",
    "        jt1 = Add()([x,c1])\n",
    "        y4 = Conv2DTranspose(64, (4, 4), activation='relu', padding='same')(jt1)\n",
    "        y5 = Conv2DTranspose(1, (4, 4), activation='relu', padding='same')(y4) \n",
    "\n",
    "        layers = y5\n",
    "\n",
    "        sym_autoencoder = Model(input_img,layers)\n",
    "        sym_autoencoder.compile(optimizer='adam', loss = ms_ssim_loss, metrics = ['accuracy',ms_ssim_loss])\n",
    "        print \"Model created\"\n",
    "    else:\n",
    "        print \"Saved model loaded\"\n",
    "    print sym_autoencoder.summary()\n",
    "    return sym_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?, ?, ?) (?, ?, ?, 1)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-4324f44b558e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msym_autoencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cnn_dsc_architecture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-48-bd8c8cdba1b9>\u001b[0m in \u001b[0;36mget_cnn_dsc_architecture\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0msym_autoencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_img\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0msym_autoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mms_ssim_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mms_ssim_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Model created\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/iplab/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    825\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m                     output_loss = weighted_loss(y_true, y_pred,\n\u001b[0;32m--> 827\u001b[0;31m                                                 sample_weight, mask)\n\u001b[0m\u001b[1;32m    828\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/iplab/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mweighted\u001b[0;34m(y_true, y_pred, weights, mask)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \"\"\"\n\u001b[1;32m    425\u001b[0m         \u001b[0;31m# score_array has ndim >= 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0mscore_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;31m# Cast the mask to floatX to avoid float64 upcasting in theano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-165639d050ba>\u001b[0m in \u001b[0;36mms_ssim_loss\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__int_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__int_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "sym_autoencoder = get_cnn_dsc_architecture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_model(sym_autoencoder, to_file='sym_model1.png',show_shapes='True',show_layer_names='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_checkpoint1 = ModelCheckpoint('./models/gated_cnn_autoencoder_oct_5layers_relu_adam_500E_8B_loss_msssim.hdf5',monitor = 'loss', verbose = 1,save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 666 samples, validate on 167 samples\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "computed output size would be negative\n\t [[Node: loss_4/conv2d_transpose_30_loss/Conv2D_22 = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](loss_4/conv2d_transpose_30_loss/mul_47, loss_4/conv2d_transpose_30_loss/div_17)]]\n\nCaused by op u'loss_4/conv2d_transpose_30_loss/Conv2D_22', defined at:\n  File \"/home/iplab/anaconda2/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/home/iplab/anaconda2/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-26-4324f44b558e>\", line 1, in <module>\n    sym_autoencoder = get_cnn_dsc_architecture()\n  File \"<ipython-input-25-1c992f82221a>\", line 62, in get_cnn_dsc_architecture\n    sym_autoencoder.compile(optimizer='adam', loss = ms_ssim_loss, metrics = ['accuracy'])\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/keras/engine/training.py\", line 827, in compile\n    sample_weight, mask)\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/keras/engine/training.py\", line 426, in weighted\n    score_array = fn(y_true, y_pred)\n  File \"<ipython-input-14-02b63526ae4b>\", line 20, in ms_ssim_loss\n    return 1- tf_ms_ssim(y_true,y_pred)\n  File \"utilities.py\", line 99, in tf_ms_ssim\n    ssim_map, cs_map = tf_ssim(img1, img2, cs_map=True, mean_metric=False)\n  File \"utilities.py\", line 78, in tf_ssim\n    sigma1_sq = tf.nn.conv2d(img1*img1, window, strides=[1,1,1,1],padding='VALID') - mu1_sq\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 396, in conv2d\n    data_format=data_format, name=name)\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): computed output size would be negative\n\t [[Node: loss_4/conv2d_transpose_30_loss/Conv2D_22 = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](loss_4/conv2d_transpose_30_loss/mul_47, loss_4/conv2d_transpose_30_loss/div_17)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-5e73a4c89726>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                 callbacks=[TensorBoard(log_dir='/tmp/gated_cnn_autoencoder', \n\u001b[1;32m      7\u001b[0m                                        \u001b[0mhistogram_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                        write_graph=True),model_checkpoint1])\n\u001b[0m",
      "\u001b[0;32m/home/iplab/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/iplab/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1204\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1206\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1207\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/iplab/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/iplab/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/iplab/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/iplab/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/iplab/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: computed output size would be negative\n\t [[Node: loss_4/conv2d_transpose_30_loss/Conv2D_22 = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](loss_4/conv2d_transpose_30_loss/mul_47, loss_4/conv2d_transpose_30_loss/div_17)]]\n\nCaused by op u'loss_4/conv2d_transpose_30_loss/Conv2D_22', defined at:\n  File \"/home/iplab/anaconda2/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/home/iplab/anaconda2/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-26-4324f44b558e>\", line 1, in <module>\n    sym_autoencoder = get_cnn_dsc_architecture()\n  File \"<ipython-input-25-1c992f82221a>\", line 62, in get_cnn_dsc_architecture\n    sym_autoencoder.compile(optimizer='adam', loss = ms_ssim_loss, metrics = ['accuracy'])\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/keras/engine/training.py\", line 827, in compile\n    sample_weight, mask)\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/keras/engine/training.py\", line 426, in weighted\n    score_array = fn(y_true, y_pred)\n  File \"<ipython-input-14-02b63526ae4b>\", line 20, in ms_ssim_loss\n    return 1- tf_ms_ssim(y_true,y_pred)\n  File \"utilities.py\", line 99, in tf_ms_ssim\n    ssim_map, cs_map = tf_ssim(img1, img2, cs_map=True, mean_metric=False)\n  File \"utilities.py\", line 78, in tf_ssim\n    sigma1_sq = tf.nn.conv2d(img1*img1, window, strides=[1,1,1,1],padding='VALID') - mu1_sq\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 396, in conv2d\n    data_format=data_format, name=name)\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/iplab/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): computed output size would be negative\n\t [[Node: loss_4/conv2d_transpose_30_loss/Conv2D_22 = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](loss_4/conv2d_transpose_30_loss/mul_47, loss_4/conv2d_transpose_30_loss/div_17)]]\n"
     ]
    }
   ],
   "source": [
    "sym_autoencoder.fit(x_train_noisy, x_train,\n",
    "                epochs = 500,\n",
    "                batch_size = 8,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test_noisy, x_test),\n",
    "                callbacks=[TensorBoard(log_dir='/tmp/gated_cnn_autoencoder', \n",
    "                                       histogram_freq=0,\n",
    "                                       write_graph=True),model_checkpoint1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Testing with gated model\n",
    "def non_ref_img_denoise_metric(noisy_img,denoised_img):\n",
    "    mni = noisy_img - denoised_img\n",
    "    ssim_n = compare_ssim(noisy_img,mni,multichannel=True)\n",
    "    ssim_p = compare_ssim(noisy_img,denoised_img,multichannel=True)\n",
    "    return pearsonr(ssim_n,ssim_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sym_autoencoder = load_model('./models/gated_oct_epoch_400_batch_8_sgd/gated_cnn_autoencoder_oct.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_images_from_folder_with_names(folder_path):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        img = cv2.imread(os.path.join(folder_path,filename),0)\n",
    "        if img is not None:\n",
    "            images.append((filename,img))\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_set = read_images_from_folder_with_names('./test_data/cropped_noisy/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288, 896, 1)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt = cv2.imread('test_data/cropped5.tif',0)\n",
    "gt = np.array(gt)\n",
    "gt.shape\n",
    "gt = cv2.resize(gt,(896,288),interpolation = cv2.INTER_CUBIC)\n",
    "gt =[gt]\n",
    "gt = np.array(gt,dtype=np.float32)\n",
    "gt = gt/255.\n",
    "gt = gt.reshape((288,896,1))\n",
    "cv2.imwrite(\"gt.tif\",gt*255.)\n",
    "gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 288, 896)\n",
      "(1, 288, 896, 1)\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "oct_shape_8_scale_7.\n",
      "21.7921047164 0.834842486071\n",
      "(1, 288, 896, 1)\n",
      "(1, 288, 896)\n",
      "(1, 288, 896, 1)\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "oct_shape_6_scale_6.\n",
      "27.3649185447 0.89871801907\n",
      "(1, 288, 896, 1)\n",
      "(1, 288, 896)\n",
      "(1, 288, 896, 1)\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "oct_shape_4_scale_3.\n",
      "28.6942525374 0.941862508866\n",
      "(1, 288, 896, 1)\n",
      "(1, 288, 896)\n",
      "(1, 288, 896, 1)\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "oct_shape_4_scale_4.\n",
      "29.4917949058 0.937623126442\n",
      "(1, 288, 896, 1)\n",
      "(1, 288, 896)\n",
      "(1, 288, 896, 1)\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "oct_shape_3_scale_2.\n",
      "27.1735782581 0.942648639919\n",
      "(1, 288, 896, 1)\n"
     ]
    }
   ],
   "source": [
    "for idx,test_image in enumerate(test_image_set):\n",
    "    test_image_norm = cv2.resize(test_image[1],(896,288),interpolation = cv2.INTER_CUBIC)\n",
    "    test_image_norm = [test_image_norm]\n",
    "    test_image_norm = np.array(test_image_norm,dtype=np.float32)\n",
    "    test_image_norm = test_image_norm/255.\n",
    "    print (test_image_norm.shape)\n",
    "    test_image_norm = test_image_norm.reshape((test_image_norm.shape[0],test_image_norm.shape[1],test_image_norm.shape[2],1))\n",
    "    print (test_image_norm.shape)\n",
    "    out_image = sym_autoencoder.predict(test_image_norm,verbose=1)\n",
    "    #print (out_image[0].shape,gt.shape)\n",
    "    psnr = compare_psnr(gt*255.,out_image[0]*255.,data_range=256)\n",
    "    ssim = compare_ssim(gt,out_image[0],multichannel=True)\n",
    "    print test_image[0][:-3]\n",
    "    print psnr,ssim\n",
    "   \n",
    "    #cv2.imwrite('./test_data/cropped_denoised_tanh/'+test_image[0][:-3] + '_'+str(round(psnr,4))+'_'+str(round(ssim,5))+'.tif',out_image[0]*255.)\n",
    "    print test_image_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Testing with simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model(\"./models/oct_epoch_500_batch_20/simple_cnn_oct.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(280, 900)\n"
     ]
    }
   ],
   "source": [
    "#Adding Gamma noise\n",
    "test_image = cv2.imread('./test_data/cropped5.tif',0)\n",
    "print test_image.shape\n",
    "#test_image = cv2.resize(test_image,(900,448),interpolation = cv2.INTER_CUBIC)\n",
    "#print test_image.shape\n",
    "gamma_param = [(3,2),(4,3),(4,4),(6,6),(8,7)]\n",
    "for param in gamma_param:\n",
    "    shape = param[0]\n",
    "    scale = param[1]\n",
    "    noise = np.random.gamma(shape,scale,test_image.shape)\n",
    "    noisy_img = np.clip(noise + test_image,0.,255.)\n",
    "    cv2.imwrite('./test_data/cropped_noisy/oct_shape_'+str(shape)+'_scale_'+str(scale)+'.tif',noisy_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_image_set = read_images_from_folder_with_names('./test_data/cropped_noisy/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s\n",
      "(280, 900, 1) (280, 900, 1)\n",
      "11.8148960657 0.39612636236\n",
      "oct_shape_8_scale_7.\n",
      "(1, 280, 900, 1)\n",
      "1/1 [==============================] - 0s\n",
      "(280, 900, 1) (280, 900, 1)\n",
      "14.9024515031 0.528854573747\n",
      "oct_shape_6_scale_6.\n",
      "(1, 280, 900, 1)\n",
      "1/1 [==============================] - 0s\n",
      "(280, 900, 1) (280, 900, 1)\n",
      "22.8196762881 0.825083118754\n",
      "oct_shape_4_scale_3.\n",
      "(1, 280, 900, 1)\n",
      "1/1 [==============================] - 0s\n",
      "(280, 900, 1) (280, 900, 1)\n",
      "20.6693131506 0.759466202918\n",
      "oct_shape_4_scale_4.\n",
      "(1, 280, 900, 1)\n",
      "1/1 [==============================] - 0s\n",
      "(280, 900, 1) (280, 900, 1)\n",
      "26.0182223411 0.888059282969\n",
      "oct_shape_3_scale_2.\n",
      "(1, 280, 900, 1)\n"
     ]
    }
   ],
   "source": [
    "for idx,test_image in enumerate(test_image_set):\n",
    "    #test_image_norm = cv2.resize(test_image,(256,128),interpolation = cv2.INTER_CUBIC)\n",
    "    test_image_norm = [test_image[1]]\n",
    "    test_image_norm = np.array(test_image_norm,dtype=np.float32)\n",
    "    test_image_norm = test_image_norm/255.\n",
    "    test_image_norm = test_image_norm.reshape((test_image_norm.shape[0],test_image_norm.shape[1],test_image_norm.shape[2],1))\n",
    "    \n",
    "    out_image = model.predict(test_image_norm,verbose=1)\n",
    "    print test_image_norm[0].shape,out_image[0].shape\n",
    "    psnr = compare_psnr(test_image_norm[0]*255.,out_image[0]*255.,data_range=256)\n",
    "    ssim = compare_ssim(test_image_norm[0],out_image[0],multichannel=True)\n",
    "    print psnr,ssim\n",
    "    print test_image[0][:-3]\n",
    "    cv2.imwrite('./test_data/cropped_denoised/'+test_image[0][:-3] + '_'+str(round(psnr,4))+'_'+str(round(ssim,5))+'.tif',out_image[0]*255.)\n",
    "    print test_image_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n"
     ]
    }
   ],
   "source": [
    "out_image = model.predict(test_image_norm,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'pack'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b665564c0c35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'pack'"
     ]
    }
   ],
   "source": [
    "tf.pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
